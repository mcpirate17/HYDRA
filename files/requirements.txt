# HYDRA Requirements
# Core dependencies
torch>=2.0.0
numpy>=1.21.0
tqdm>=4.62.0
pyyaml>=6.0

# Tokenization
tiktoken>=0.5.0
transformers>=4.30.0

# Dataset loading
datasets>=2.14.0

# Logging and monitoring
wandb>=0.15.0
tensorboard>=2.13.0

# Testing
pytest>=7.0.0
pytest-cov>=4.0.0

# =============================================================================
# OPTIONAL HIGH-PERFORMANCE DEPENDENCIES
# Uncomment based on your GPU and requirements
# =============================================================================

# Triton (fused kernels) - HIGHLY RECOMMENDED for NVIDIA GPUs
# Triton 3.0+ recommended for best autotuning
triton>=2.0.0

# Flash Attention 2 - RECOMMENDED for long sequences
# Requires CUDA 11.6+ and compute capability 8.0+ (Ampere+)
# Install with: pip install flash-attn --no-build-isolation
# flash-attn>=2.5.0

# xFormers - Alternative memory-efficient attention
# Good fallback if Flash Attention doesn't work
# pip install xformers
# xformers>=0.0.22

# Liger Kernel - Drop-in fused kernels (alternative to our Triton)
# https://github.com/linkedin/Liger-Kernel
# pip install liger-kernel
# liger-kernel>=0.1.0

# bitsandbytes - 8-bit optimizers and quantization
# Useful for memory-constrained training
# pip install bitsandbytes
# bitsandbytes>=0.41.0

# DeepSpeed - Distributed training and ZeRO optimization
# pip install deepspeed
# deepspeed>=0.10.0

# =============================================================================
# DEVELOPMENT DEPENDENCIES
# =============================================================================

# Code formatting
black>=23.0.0
isort>=5.12.0

# Type checking
mypy>=1.0.0

# Profiling
line_profiler>=4.0.0
memory_profiler>=0.60.0
py-spy>=0.3.14

# Documentation
sphinx>=6.0.0
sphinx-rtd-theme>=1.2.0
