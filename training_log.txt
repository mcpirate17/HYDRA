nohup: ignoring input
E1214 17:28:13.958000 102645 torch/_inductor/select_algorithm.py:2100] [9/0] Runtime error during autotuning: 
E1214 17:28:13.958000 102645 torch/_inductor/select_algorithm.py:2100] [9/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. 
E1214 17:28:13.958000 102645 torch/_inductor/select_algorithm.py:2100] [9/0] Ignoring this choice.
E1214 17:28:14.095000 102645 torch/_inductor/select_algorithm.py:2100] [9/0] Runtime error during autotuning: 
E1214 17:28:14.095000 102645 torch/_inductor/select_algorithm.py:2100] [9/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. 
E1214 17:28:14.095000 102645 torch/_inductor/select_algorithm.py:2100] [9/0] Ignoring this choice.
E1214 17:28:14.332000 102645 torch/_inductor/select_algorithm.py:2100] [9/0] Runtime error during autotuning: 
E1214 17:28:14.332000 102645 torch/_inductor/select_algorithm.py:2100] [9/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. 
E1214 17:28:14.332000 102645 torch/_inductor/select_algorithm.py:2100] [9/0] Ignoring this choice.
AUTOTUNE mm(16384x4608, 4608x1280)
  mm 0.8253 ms 100.0% 
  triton_mm_16 0.8335 ms 99.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_9 0.8561 ms 96.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_13 0.8581 ms 96.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_15 0.8584 ms 96.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=8
  triton_mm_11 0.8602 ms 95.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_17 0.8704 ms 94.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_10 0.8843 ms 93.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_14 0.8888 ms 92.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_7 0.9073 ms 91.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
SingleProcess AUTOTUNE benchmarking takes 0.8005 seconds and 0.0173 seconds precompiling for 20 choices
E1214 17:28:22.939000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Runtime error during autotuning: 
E1214 17:28:22.939000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. 
E1214 17:28:22.939000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Ignoring this choice.
E1214 17:28:23.061000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Runtime error during autotuning: 
E1214 17:28:23.061000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. 
E1214 17:28:23.061000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Ignoring this choice.
E1214 17:28:23.279000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Runtime error during autotuning: 
E1214 17:28:23.279000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. 
E1214 17:28:23.279000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Ignoring this choice.
AUTOTUNE mm(320x16384, 16384x1280)
  mm 0.0737 ms 100.0% 
  triton_mm_155 0.1240 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_150 0.1307 ms 56.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_149 0.1320 ms 55.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_151 0.1362 ms 54.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_152 0.1372 ms 53.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_159 0.2022 ms 36.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_162 0.2024 ms 36.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_158 0.2109 ms 35.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_157 0.2127 ms 34.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6585 seconds and 0.0032 seconds precompiling for 20 choices
E1214 17:28:23.595000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Runtime error during autotuning: 
E1214 17:28:23.595000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. 
E1214 17:28:23.595000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Ignoring this choice.
E1214 17:28:23.722000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Runtime error during autotuning: 
E1214 17:28:23.722000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. 
E1214 17:28:23.722000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Ignoring this choice.
E1214 17:28:23.935000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Runtime error during autotuning: 
E1214 17:28:23.935000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. 
E1214 17:28:23.935000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Ignoring this choice.
AUTOTUNE mm(1280x16384, 16384x320)
  mm 0.0816 ms 100.0% 
  triton_mm_64 0.1285 ms 63.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_59 0.1310 ms 62.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_58 0.1348 ms 60.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_60 0.1372 ms 59.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_61 0.1479 ms 55.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_68 0.2023 ms 40.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_71 0.2039 ms 40.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_67 0.2044 ms 39.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_66 0.2144 ms 38.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6514 seconds and 0.0002 seconds precompiling for 20 choices
E1214 17:28:24.238000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Runtime error during autotuning: 
E1214 17:28:24.238000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. 
E1214 17:28:24.238000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Ignoring this choice.
E1214 17:28:24.360000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Runtime error during autotuning: 
E1214 17:28:24.360000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. 
E1214 17:28:24.360000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Ignoring this choice.
E1214 17:28:24.569000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Runtime error during autotuning: 
E1214 17:28:24.569000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. 
E1214 17:28:24.569000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Ignoring this choice.
AUTOTUNE mm(80x16384, 16384x1280)
  mm 0.0447 ms 100.0% 
  triton_mm_133 0.0718 ms 62.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_131 0.0849 ms 52.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_132 0.0882 ms 50.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_130 0.1100 ms 40.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_136 0.1283 ms 34.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_140 0.2003 ms 22.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_143 0.2020 ms 22.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_139 0.2040 ms 21.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_142 0.2129 ms 21.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.6278 seconds and 0.0001 seconds precompiling for 20 choices
E1214 17:28:24.880000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Runtime error during autotuning: 
E1214 17:28:24.880000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. 
E1214 17:28:24.880000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Ignoring this choice.
E1214 17:28:25.002000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Runtime error during autotuning: 
E1214 17:28:25.002000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. 
E1214 17:28:25.002000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Ignoring this choice.
AUTOTUNE mm(40x16384, 16384x1280)
  mm 0.0610 ms 100.0% 
  triton_mm_97 0.0781 ms 78.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_99 0.0816 ms 74.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_98 0.0954 ms 63.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_96 0.1188 ms 51.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_109 0.1309 ms 46.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_102 0.1319 ms 46.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_108 0.1818 ms 33.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_111 0.1941 ms 31.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_106 0.2023 ms 30.1% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5691 seconds and 0.0001 seconds precompiling for 18 choices
E1214 17:28:25.441000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Runtime error during autotuning: 
E1214 17:28:25.441000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. 
E1214 17:28:25.441000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Ignoring this choice.
E1214 17:28:25.563000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Runtime error during autotuning: 
E1214 17:28:25.563000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. 
E1214 17:28:25.563000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Ignoring this choice.
AUTOTUNE mm(40x16384, 16384x1280)
  mm 0.0631 ms 100.0% 
  triton_mm_116 0.0815 ms 77.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=4
  triton_mm_114 0.0853 ms 74.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_115 0.0938 ms 67.2% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_113 0.1126 ms 56.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=128, BLOCK_M=32, BLOCK_N=32, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_126 0.1323 ms 47.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_119 0.1350 ms 46.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_125 0.1837 ms 34.3% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_128 0.1941 ms 32.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  triton_mm_123 0.2000 ms 31.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.5739 seconds and 0.0001 seconds precompiling for 18 choices
E1214 17:28:26.002000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Runtime error during autotuning: 
E1214 17:28:26.002000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. 
E1214 17:28:26.002000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Ignoring this choice.
E1214 17:28:26.082000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Runtime error during autotuning: 
E1214 17:28:26.082000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 147456, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. 
E1214 17:28:26.082000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Ignoring this choice.
E1214 17:28:26.213000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Runtime error during autotuning: 
E1214 17:28:26.213000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] No valid triton configs. OutOfResources: out of resource: shared memory, Required: 131072, Hardware limit: 101376. Reducing block sizes or `num_stages` may help.. 
E1214 17:28:26.213000 102645 torch/_inductor/select_algorithm.py:2100] [6/0] Ignoring this choice.
AUTOTUNE mm(16384x1280, 1280x320)
  triton_mm_83 0.0676 ms 100.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=8
  triton_mm_89 0.0700 ms 96.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_90 0.0752 ms 89.8% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_85 0.0799 ms 84.6% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_87 0.0815 ms 82.9% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=64, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
  triton_mm_86 0.0856 ms 79.0% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=4, num_warps=8
  triton_mm_82 0.0884 ms 76.4% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=64, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=2, num_warps=4
  triton_mm_78 0.0942 ms 71.7% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=32, BLOCK_N=64, EVEN_K=True, GROUP_M=8, num_stages=5, num_warps=8
  mm 0.0957 ms 70.6% 
  triton_mm_92 0.0958 ms 70.5% ACC_TYPE='tl.float32', ALLOW_TF32=False, BLOCK_K=32, BLOCK_M=128, BLOCK_N=128, EVEN_K=True, GROUP_M=8, num_stages=3, num_warps=4
SingleProcess AUTOTUNE benchmarking takes 0.4707 seconds and 0.0001 seconds precompiling for 20 choices

============================================================
TRAINING MODE: TESTING
============================================================
  Max steps:      5,000
  Warmup steps:   150
  Decay starts:   3,500
  Decay steps:    1,500
  Save interval:  500
  Sequence len:   512 (fixed)
  Total tokens:   327.7M
  Architecture:   VANILLA (HybridTransformer)
                  dim=768, 3 macro-blocks √ó 8 = 24 layers
                  ~215M params
============================================================


‚ö†Ô∏è  DIAGNOSTIC RUN: max_steps=100000, save_interval=500
Checkpoint RoPE cache seq_len: 512
Checkpoint architecture: dim=1280, blocks=8, recursions=3, heads=20
Overriding architecture: vanilla -> mod_mor (from checkpoint)
Checkpoint final LR: 0.000150
Creating model with max_seq_len=512 (covers all training phases)
MoD: 50% capacity
MoR: adaptive, 3 recursions/block
MoR RESTART MODE: Adaptive routing enabled from start (resumed after enable point)
Compiling model with mode='max-autotune-no-cudagraphs'...
Model compiled successfully!
Loading finefineweb dataset...
Stepped sequence schedule: () + final @ 512
Starting with seq_len=512
Tokens per step: 65,536
Dataset ready!

======================================================================
RESUMING FROM CHECKPOINT: checkpoints/old/hydra_100m_step_84500.pt
======================================================================
  Loaded step: 84500
  Previous best loss: 2.5415271520614624
  Previous total tokens: 1,384,448,000
  Will continue to step: 100000
  Remaining steps: 15,500
======================================================================


======================================================================
HYDRA 100M Optimized Trainer
======================================================================
Device: cuda
Model: 220.2M parameters
Batch: 32 micro √ó 4 accum = 128 effective
Sequence length: 512
Tokens/step: 65,536 (0.07M per optimizer step)
Dataset: finefineweb
torch.compile: True (mode=max-autotune-no-cudagraphs)
AMP dtype: bfloat16
LR Schedule: WSD (Warmup-Stable-Decay)
  Warmup: 150 steps (0.1%)
  Stable: 3350 steps (3.4%) at LR=0.0005
  Decay:  1500 steps (1.5%) -> LR=0.00015
======================================================================

Resuming training from step 84500...
----------------------------------------------------------------------
Resume loss: 4.4158 (previous best: 2.5415)
Step 84525/100000 | Loss: 4.2957 (EMA: 4.3454) | LR: 1.50e-04 | Grad: 0.73 | 85.0K tok/s | Avg: 34.9K tok/s (0.53 steps/s)
Step 84550/100000 | Loss: 4.1355 (EMA: 4.2742) | LR: 1.50e-04 | Grad: 0.67 | 86.9K tok/s | Avg: 50.0K tok/s (0.76 steps/s)
Step 84575/100000 | Loss: 4.1135 (EMA: 4.1723) | LR: 1.50e-04 | Grad: 0.67 | 87.2K tok/s | Avg: 58.6K tok/s (0.89 steps/s)
Step 84600/100000 | Loss: 4.0032 (EMA: 4.0826) | LR: 1.50e-04 | Grad: 0.69 | 87.9K tok/s | Avg: 64.1K tok/s (0.98 steps/s)
Step 84625/100000 | Loss: 3.9749 (EMA: 4.0295) | LR: 1.50e-04 | Grad: 0.62 | 95.5K tok/s | Avg: 67.9K tok/s (1.04 steps/s)
Step 84650/100000 | Loss: 4.1648 (EMA: 4.0351) | LR: 1.50e-04 | Grad: 0.67 | 85.3K tok/s | Avg: 70.6K tok/s (1.08 steps/s)
Step 84675/100000 | Loss: 4.1006 (EMA: 4.0507) | LR: 1.50e-04 | Grad: 0.73 | 76.1K tok/s | Avg: 72.7K tok/s (1.11 steps/s)
Step 84700/100000 | Loss: 4.1237 (EMA: 4.0967) | LR: 1.50e-04 | Grad: 0.66 | 88.1K tok/s | Avg: 74.5K tok/s (1.14 steps/s)
Step 84725/100000 | Loss: 4.1764 (EMA: 4.1381) | LR: 1.50e-04 | Grad: 0.67 | 85.4K tok/s | Avg: 75.6K tok/s (1.15 steps/s)
Step 84750/100000 | Loss: 4.2796 (EMA: 4.2472) | LR: 1.50e-04 | Grad: 0.66 | 87.6K tok/s | Avg: 76.7K tok/s (1.17 steps/s)
Step 84775/100000 | Loss: 4.4500 (EMA: 4.3439) | LR: 1.50e-04 | Grad: 0.66 | 88.3K tok/s | Avg: 77.7K tok/s (1.18 steps/s)
Step 84800/100000 | Loss: 4.4533 (EMA: 4.4459) | LR: 1.50e-04 | Grad: 0.68 | 94.2K tok/s | Avg: 78.4K tok/s (1.20 steps/s)
Step 84825/100000 | Loss: 4.6972 (EMA: 4.5887) | LR: 1.50e-04 | Grad: 0.78 | 87.4K tok/s | Avg: 79.1K tok/s (1.21 steps/s)
Step 84850/100000 | Loss: 4.9080 (EMA: 4.8022) | LR: 1.50e-04 | Grad: 0.71 | 86.4K tok/s | Avg: 79.6K tok/s (1.21 steps/s)
Step 84875/100000 | Loss: 5.0689 (EMA: 4.9535) | LR: 1.50e-04 | Grad: 0.72 | 83.9K tok/s | Avg: 80.1K tok/s (1.22 steps/s)
Step 84900/100000 | Loss: 5.1525 (EMA: 5.0842) | LR: 1.50e-04 | Grad: 0.72 | 94.8K tok/s | Avg: 80.5K tok/s (1.23 steps/s)
Step 84925/100000 | Loss: 5.3777 (EMA: 5.2204) | LR: 1.50e-04 | Grad: 0.78 | 86.8K tok/s | Avg: 80.9K tok/s (1.23 steps/s)
Step 84950/100000 | Loss: 5.4378 (EMA: 5.4164) | LR: 1.50e-04 | Grad: 0.77 | 88.5K tok/s | Avg: 81.3K tok/s (1.24 steps/s)
Step 84975/100000 | Loss: 5.6430 (EMA: 5.4732) | LR: 1.50e-04 | Grad: 0.78 | 86.4K tok/s | Avg: 81.6K tok/s (1.25 steps/s)
[EVAL] step=85000  eval_loss=5.5760  train_loss=5.4572
Step 85000/100000 | Loss: 5.4572 (EMA: 5.4827) | LR: 1.50e-04 | Grad: 0.81 | 87.0K tok/s | Avg: 81.3K tok/s (1.24 steps/s)
  [DIAG] MoD:HARD save=50% | MoR:FULL d=0.60 [68/5/27%] | CE=5.365 aux=0.0000 ponder=0.097
üìä Diagnostics saved to checkpoints/training_diagnostics.json
Checkpoint saved: checkpoints/hydra_100m_step_85000.pt
Step 85025/100000 | Loss: 5.6854 (EMA: 5.5963) | LR: 1.50e-04 | Grad: 0.85 | 84.4K tok/s | Avg: 81.3K tok/s (1.24 steps/s)
Step 85050/100000 | Loss: 5.7503 (EMA: 5.7013) | LR: 1.50e-04 | Grad: 0.77 | 94.8K tok/s | Avg: 81.6K tok/s (1.25 steps/s)
Step 85075/100000 | Loss: 5.9434 (EMA: 5.8093) | LR: 1.50e-04 | Grad: 0.88 | 85.5K tok/s | Avg: 81.8K tok/s (1.25 steps/s)
Step 85100/100000 | Loss: 6.0515 (EMA: 5.9608) | LR: 1.50e-04 | Grad: 0.80 | 87.1K tok/s | Avg: 82.0K tok/s (1.25 steps/s)
Step 85125/100000 | Loss: 6.0607 (EMA: 6.0378) | LR: 1.50e-04 | Grad: 0.79 | 99.8K tok/s | Avg: 82.2K tok/s (1.25 steps/s)
Step 85150/100000 | Loss: 5.9848 (EMA: 5.9850) | LR: 1.50e-04 | Grad: 0.84 | 90.2K tok/s | Avg: 82.5K tok/s (1.26 steps/s)
Step 85175/100000 | Loss: 6.0784 (EMA: 6.0238) | LR: 1.50e-04 | Grad: 0.84 | 90.8K tok/s | Avg: 82.8K tok/s (1.26 steps/s)
Step 85200/100000 | Loss: 5.9915 (EMA: 6.0023) | LR: 1.50e-04 | Grad: 0.94 | 89.4K tok/s | Avg: 83.1K tok/s (1.27 steps/s)
Step 85225/100000 | Loss: 6.2508 (EMA: 6.1992) | LR: 1.50e-04 | Grad: 0.87 | 89.3K tok/s | Avg: 83.3K tok/s (1.27 steps/s)
Step 85250/100000 | Loss: 6.4111 (EMA: 6.3145) | LR: 1.50e-04 | Grad: 0.99 | 84.4K tok/s | Avg: 83.6K tok/s (1.28 steps/s)
Step 85275/100000 | Loss: 6.7325 (EMA: 6.5220) | LR: 1.50e-04 | Grad: 0.83 | 91.7K tok/s | Avg: 83.8K tok/s (1.28 steps/s)
Step 85300/100000 | Loss: 6.9893 (EMA: 6.7561) | LR: 1.50e-04 | Grad: 0.88 | 100.0K tok/s | Avg: 84.1K tok/s (1.28 steps/s)
Step 85325/100000 | Loss: 8.3277 (EMA: 7.4674) | LR: 1.50e-04 | Grad: 1.10 | 100.7K tok/s | Avg: 84.3K tok/s (1.29 steps/s)
Step 85350/100000 | Loss: 8.7363 (EMA: 8.3192) | LR: 1.50e-04 | Grad: 0.92 | 91.9K tok/s | Avg: 84.5K tok/s (1.29 steps/s)
Step 85375/100000 | Loss: 8.2326 (EMA: 8.4288) | LR: 1.50e-04 | Grad: 1.08 | 90.4K tok/s | Avg: 84.7K tok/s (1.29 steps/s)
Step 85400/100000 | Loss: 7.9258 (EMA: 8.1049) | LR: 1.50e-04 | Grad: 1.12 | 90.8K tok/s | Avg: 84.9K tok/s (1.30 steps/s)
Step 85425/100000 | Loss: 7.9942 (EMA: 8.0194) | LR: 1.50e-04 | Grad: 0.94 | 100.6K tok/s | Avg: 85.1K tok/s (1.30 steps/s)
Step 85450/100000 | Loss: 7.7639 (EMA: 7.8791) | LR: 1.50e-04 | Grad: 0.95 | 90.5K tok/s | Avg: 85.2K tok/s (1.30 steps/s)
Step 85475/100000 | Loss: 7.3995 (EMA: 7.5979) | LR: 1.50e-04 | Grad: 1.12 | 99.5K tok/s | Avg: 85.4K tok/s (1.30 steps/s)
[EVAL] step=85500  eval_loss=7.4575  train_loss=7.3827
Step 85500/100000 | Loss: 7.3827 (EMA: 7.4145) | LR: 1.50e-04 | Grad: 1.05 | 90.5K tok/s | Avg: 85.2K tok/s (1.30 steps/s)
  [DIAG] MoD:HARD save=50% | MoR:FULL d=0.62 [69/1/31%] | CE=7.370 aux=0.0000 ponder=0.089
üìä Diagnostics saved to checkpoints/training_diagnostics.json
Checkpoint saved: checkpoints/hydra_100m_step_85500.pt
   ‚ö†Ô∏è  Loss increased: 5.4572 ‚Üí 7.3827 (+35.3%) [1/3]
       (Chinchilla progress: 127.2%, early stop active)
Step 85525/100000 | Loss: 7.7449 (EMA: 7.5243) | LR: 1.50e-04 | Grad: 0.98 | 100.8K tok/s | Avg: 85.2K tok/s (1.30 steps/s)
