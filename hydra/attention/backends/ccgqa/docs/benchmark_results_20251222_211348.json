{
  "timestamp": "2025-12-22T21:13:48.024131",
  "gpu_info": {
    "name": "NVIDIA GeForce RTX 5090",
    "compute_capability": "12.0",
    "memory_gb": 33.7
  },
  "config": {
    "batch_size": 4,
    "n_heads": 8,
    "n_kv_heads": 2,
    "head_dim": 64,
    "dim": 512,
    "compression_factor": 4,
    "dtype": "torch.bfloat16",
    "warmup_iters": 20,
    "bench_iters": 50,
    "seq_lengths": [
      512,
      1024,
      2048,
      4096,
      8192,
      16384
    ],
    "device": "cuda"
  },
  "results": [
    {
      "kernel": "CCGQA Original",
      "fwd_time_ms": 0.4552,
      "bwd_time_ms": 0.7226,
      "total_time_ms": 1.1778,
      "throughput_iters_per_sec": 849.0,
      "peak_memory_mb": 34.9,
      "success": true,
      "seq_len": 512
    },
    {
      "kernel": "CCGQA2",
      "fwd_time_ms": 0.4307,
      "bwd_time_ms": 0.7199,
      "total_time_ms": 1.1506,
      "throughput_iters_per_sec": 869.1,
      "peak_memory_mb": 34.3,
      "success": true,
      "seq_len": 512
    },
    {
      "kernel": "GQA-Ref",
      "fwd_time_ms": 0.1227,
      "bwd_time_ms": 0.2815,
      "total_time_ms": 0.4042,
      "throughput_iters_per_sec": 2473.8,
      "peak_memory_mb": 99.1,
      "success": true,
      "seq_len": 512
    },
    {
      "kernel": "MHA-Ref",
      "fwd_time_ms": 0.1613,
      "bwd_time_ms": 0.3037,
      "total_time_ms": 0.465,
      "throughput_iters_per_sec": 2150.3,
      "peak_memory_mb": 103.3,
      "success": true,
      "seq_len": 512
    },
    {
      "kernel": "PyTorch-SDPA",
      "fwd_time_ms": 0.0447,
      "bwd_time_ms": 0.0661,
      "total_time_ms": 0.1108,
      "throughput_iters_per_sec": 9029.1,
      "peak_memory_mb": 61.5,
      "success": true,
      "seq_len": 512
    },
    {
      "kernel": "FlashAttn-2",
      "fwd_time_ms": 0.0529,
      "bwd_time_ms": 0.0898,
      "total_time_ms": 0.1428,
      "throughput_iters_per_sec": 7004.3,
      "peak_memory_mb": 57.3,
      "success": true,
      "seq_len": 512
    },
    {
      "kernel": "CCGQA Original",
      "fwd_time_ms": 0.4592,
      "bwd_time_ms": 0.8475,
      "total_time_ms": 1.3067,
      "throughput_iters_per_sec": 765.3,
      "peak_memory_mb": 62.1,
      "success": true,
      "seq_len": 1024
    },
    {
      "kernel": "CCGQA2",
      "fwd_time_ms": 0.428,
      "bwd_time_ms": 0.8107,
      "total_time_ms": 1.2387,
      "throughput_iters_per_sec": 807.3,
      "peak_memory_mb": 59.0,
      "success": true,
      "seq_len": 1024
    },
    {
      "kernel": "GQA-Ref",
      "fwd_time_ms": 0.3468,
      "bwd_time_ms": 0.7227,
      "total_time_ms": 1.0696,
      "throughput_iters_per_sec": 935.0,
      "peak_memory_mb": 315.2,
      "success": true,
      "seq_len": 1024
    },
    {
      "kernel": "MHA-Ref",
      "fwd_time_ms": 0.411,
      "bwd_time_ms": 0.7603,
      "total_time_ms": 1.1713,
      "throughput_iters_per_sec": 853.8,
      "peak_memory_mb": 315.2,
      "success": true,
      "seq_len": 1024
    },
    {
      "kernel": "PyTorch-SDPA",
      "fwd_time_ms": 0.1044,
      "bwd_time_ms": 0.1406,
      "total_time_ms": 0.245,
      "throughput_iters_per_sec": 4081.5,
      "peak_memory_mb": 97.3,
      "success": true,
      "seq_len": 1024
    },
    {
      "kernel": "FlashAttn-2",
      "fwd_time_ms": 0.0816,
      "bwd_time_ms": 0.185,
      "total_time_ms": 0.2666,
      "throughput_iters_per_sec": 3751.5,
      "peak_memory_mb": 89.0,
      "success": true,
      "seq_len": 1024
    },
    {
      "kernel": "CCGQA Original",
      "fwd_time_ms": 0.5348,
      "bwd_time_ms": 0.9858,
      "total_time_ms": 1.5206,
      "throughput_iters_per_sec": 657.7,
      "peak_memory_mb": 94.5,
      "success": true,
      "seq_len": 2048
    },
    {
      "kernel": "CCGQA2",
      "fwd_time_ms": 0.5035,
      "bwd_time_ms": 0.9693,
      "total_time_ms": 1.4728,
      "throughput_iters_per_sec": 679.0,
      "peak_memory_mb": 88.3,
      "success": true,
      "seq_len": 2048
    },
    {
      "kernel": "GQA-Ref",
      "fwd_time_ms": 1.4255,
      "bwd_time_ms": 2.8948,
      "total_time_ms": 4.3203,
      "throughput_iters_per_sec": 231.5,
      "peak_memory_mb": 1141.6,
      "success": true,
      "seq_len": 2048
    },
    {
      "kernel": "MHA-Ref",
      "fwd_time_ms": 1.5754,
      "bwd_time_ms": 2.9606,
      "total_time_ms": 4.536,
      "throughput_iters_per_sec": 220.5,
      "peak_memory_mb": 1141.6,
      "success": true,
      "seq_len": 2048
    },
    {
      "kernel": "PyTorch-SDPA",
      "fwd_time_ms": 0.2584,
      "bwd_time_ms": 0.5752,
      "total_time_ms": 0.8336,
      "throughput_iters_per_sec": 1199.6,
      "peak_memory_mb": 169.0,
      "success": true,
      "seq_len": 2048
    },
    {
      "kernel": "FlashAttn-2",
      "fwd_time_ms": 0.2523,
      "bwd_time_ms": 0.5572,
      "total_time_ms": 0.8095,
      "throughput_iters_per_sec": 1235.3,
      "peak_memory_mb": 152.3,
      "success": true,
      "seq_len": 2048
    },
    {
      "kernel": "CCGQA Original",
      "fwd_time_ms": 0.6892,
      "bwd_time_ms": 1.5036,
      "total_time_ms": 2.1928,
      "throughput_iters_per_sec": 456.0,
      "peak_memory_mb": 165.8,
      "success": true,
      "seq_len": 4096
    },
    {
      "kernel": "CCGQA2",
      "fwd_time_ms": 0.6808,
      "bwd_time_ms": 1.5162,
      "total_time_ms": 2.1971,
      "throughput_iters_per_sec": 455.2,
      "peak_memory_mb": 153.2,
      "success": true,
      "seq_len": 4096
    },
    {
      "kernel": "GQA-Ref",
      "fwd_time_ms": 5.3323,
      "bwd_time_ms": 9.7014,
      "total_time_ms": 15.0337,
      "throughput_iters_per_sec": 66.5,
      "peak_memory_mb": 4405.0,
      "success": true,
      "seq_len": 4096
    },
    {
      "kernel": "MHA-Ref",
      "fwd_time_ms": 5.5954,
      "bwd_time_ms": 9.8618,
      "total_time_ms": 15.4572,
      "throughput_iters_per_sec": 64.7,
      "peak_memory_mb": 4405.0,
      "success": true,
      "seq_len": 4096
    },
    {
      "kernel": "PyTorch-SDPA",
      "fwd_time_ms": 0.7909,
      "bwd_time_ms": 1.9354,
      "total_time_ms": 2.7263,
      "throughput_iters_per_sec": 366.8,
      "peak_memory_mb": 312.4,
      "success": true,
      "seq_len": 4096
    },
    {
      "kernel": "FlashAttn-2",
      "fwd_time_ms": 0.7747,
      "bwd_time_ms": 1.9079,
      "total_time_ms": 2.6826,
      "throughput_iters_per_sec": 372.8,
      "peak_memory_mb": 278.9,
      "success": true,
      "seq_len": 4096
    },
    {
      "kernel": "CCGQA Original",
      "fwd_time_ms": 1.4019,
      "bwd_time_ms": 3.4095,
      "total_time_ms": 4.8114,
      "throughput_iters_per_sec": 207.8,
      "peak_memory_mb": 297.5,
      "success": true,
      "seq_len": 8192
    },
    {
      "kernel": "CCGQA2",
      "fwd_time_ms": 1.3836,
      "bwd_time_ms": 3.3843,
      "total_time_ms": 4.7679,
      "throughput_iters_per_sec": 209.7,
      "peak_memory_mb": 272.6,
      "success": true,
      "seq_len": 8192
    },
    {
      "kernel": "GQA-Ref",
      "fwd_time_ms": 20.0752,
      "bwd_time_ms": 37.5881,
      "total_time_ms": 57.6633,
      "throughput_iters_per_sec": 17.3,
      "peak_memory_mb": 17374.3,
      "success": true,
      "seq_len": 8192
    },
    {
      "kernel": "MHA-Ref",
      "fwd_time_ms": 20.5382,
      "bwd_time_ms": 37.6026,
      "total_time_ms": 58.1408,
      "throughput_iters_per_sec": 17.2,
      "peak_memory_mb": 17374.3,
      "success": true,
      "seq_len": 8192
    },
    {
      "kernel": "PyTorch-SDPA",
      "fwd_time_ms": 2.7796,
      "bwd_time_ms": 7.0548,
      "total_time_ms": 9.8344,
      "throughput_iters_per_sec": 101.7,
      "peak_memory_mb": 599.2,
      "success": true,
      "seq_len": 8192
    },
    {
      "kernel": "FlashAttn-2",
      "fwd_time_ms": 2.7504,
      "bwd_time_ms": 6.9343,
      "total_time_ms": 9.6847,
      "throughput_iters_per_sec": 103.3,
      "peak_memory_mb": 532.1,
      "success": true,
      "seq_len": 8192
    },
    {
      "kernel": "CCGQA Original",
      "fwd_time_ms": 3.8967,
      "bwd_time_ms": 9.5682,
      "total_time_ms": 13.4649,
      "throughput_iters_per_sec": 74.3,
      "peak_memory_mb": 569.4,
      "success": true,
      "seq_len": 16384
    },
    {
      "kernel": "CCGQA2",
      "fwd_time_ms": 3.8689,
      "bwd_time_ms": 9.5327,
      "total_time_ms": 13.4016,
      "throughput_iters_per_sec": 74.6,
      "peak_memory_mb": 519.5,
      "success": true,
      "seq_len": 16384
    },
    {
      "kernel": "GQA-Ref",
      "error": "CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 31.36 GiB of which 13.75 GiB is free. Including non-PyTorch memory, this process has 17.17 GiB memory in use. Of the allocated memory 16.34 GiB is allocated by PyTorch, and 220.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
      "success": false,
      "seq_len": 16384
    },
    {
      "kernel": "MHA-Ref",
      "error": "CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 31.36 GiB of which 13.75 GiB is free. Including non-PyTorch memory, this process has 17.17 GiB memory in use. Of the allocated memory 16.40 GiB is allocated by PyTorch, and 158.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
      "success": false,
      "seq_len": 16384
    },
    {
      "kernel": "PyTorch-SDPA",
      "fwd_time_ms": 10.7035,
      "bwd_time_ms": 27.2333,
      "total_time_ms": 37.9368,
      "throughput_iters_per_sec": 26.4,
      "peak_memory_mb": 1169.4,
      "success": true,
      "seq_len": 16384
    },
    {
      "kernel": "FlashAttn-2",
      "fwd_time_ms": 10.6142,
      "bwd_time_ms": 27.1866,
      "total_time_ms": 37.8008,
      "throughput_iters_per_sec": 26.5,
      "peak_memory_mb": 1035.2,
      "success": true,
      "seq_len": 16384
    }
  ]
}