{
  "metadata": {
    "timestamp": "20251213_172418",
    "model": "HYDRA 100M",
    "dataset": "finefineweb",
    "device": "cuda",
    "torch_version": "2.7.1+cu128",
    "cuda_version": "12.8"
  },
  "configuration": {
    "mode": "testing",
    "resume_from": "checkpoints/hydra_100m_final.pt",
    "start_step": 0,
    "architecture": "mod_mor",
    "mod_capacity": 0.5,
    "mor_adaptive": true,
    "mor_enable_pct": 0.3,
    "mor_rampup_steps": 1000,
    "mor_already_enabled": true,
    "aux_scale": 0.1,
    "ponder_scale": 0.0,
    "dim": 768,
    "n_macro_blocks": 3,
    "n_heads": 12,
    "n_kv_heads": 3,
    "mod_mor_dim": 1280,
    "n_mor_blocks": 8,
    "mor_recursions": 3,
    "mod_mor_n_heads": 20,
    "mod_mor_n_kv_heads": 5,
    "vocab_size": 50257,
    "max_seq_len": 512,
    "max_steps": 20000,
    "warmup_steps": 150,
    "decay_start_step": 3500,
    "decay_steps": 1500,
    "seq_steps": [],
    "save_interval": 500,
    "batch_size": 8,
    "grad_accum_steps": 4,
    "max_lr": 0.0005,
    "min_lr": 0.00015,
    "weight_decay": 0.1,
    "grad_clip": 1.0,
    "lr_schedule": "wsd",
    "dataset_name": "finefineweb",
    "tokenizer_name": "gpt2",
    "use_compile": true,
    "compile_mode": "max-autotune-no-cudagraphs",
    "dtype": "bfloat16",
    "log_interval": 25,
    "eval_interval": 100,
    "max_checkpoints": 3,
    "early_stop_patience": 3,
    "early_stop_threshold": 0.1,
    "chinchilla_multiplier": 20.0,
    "early_stop_min_progress": 0.5,
    "checkpoint_dir": "checkpoints",
    "report_dir": "reports"
  },
  "training_summary": {
    "total_steps": 20000,
    "total_tokens": 716800000,
    "training_time_seconds": 9.5367431640625e-07,
    "training_time_formatted": "0.0s"
  },
  "loss_analysis": {
    "initial_loss": 0.0,
    "final_loss": 0.0,
    "best_loss": 2.5415271520614624,
    "best_loss_step": 26295,
    "loss_reduction_percent": 0,
    "average_loss": 0,
    "min_loss": 0,
    "max_loss": 0,
    "loss_at_25_percent": 0,
    "loss_at_50_percent": 0,
    "loss_at_75_percent": 0
  },
  "performance": {
    "average_tokens_per_second": 751619276800000.0,
    "average_tokens_per_second_steady": 0,
    "peak_tokens_per_second": 0,
    "average_step_time_ms": 0
  },
  "gradient_analysis": {
    "average_grad_norm": 0,
    "max_grad_norm": 0,
    "min_grad_norm": 0
  },
  "model_assessment": {
    "learning_quality": "Poor - May need more steps or tuning",
    "vs_random_baseline": "100.0% better - Approaching usable"
  },
  "training_assessment": {},
  "raw_metrics": {
    "losses": [],
    "learning_rates": [],
    "grad_norms": [],
    "tokens_per_sec": [],
    "step_times": [],
    "start_time": 1765664656.39594,
    "end_time": 1765664656.395941,
    "total_tokens": 716800000,
    "best_loss": 2.5415271520614624,
    "best_loss_step": 26295,
    "initial_loss": 0.0,
    "final_loss": 0.0,
    "training_time_seconds": 9.5367431640625e-07
  }
}