====================================================================================================
COMPREHENSIVE CCGQA DIAGNOSTICS REPORT
====================================================================================================

Generated: 2025-12-07 14:46:44
Device: cuda


====================================================================================================
TEST: 01_attention_layer
====================================================================================================

MODEL CONFIGURATION
----------------------------------------------------------------------------------------------------
  dim: 768
  n_heads: 12
  n_kv_heads: 3
  compression_factor: 4
  use_convs: True
  use_qk_mean: True
  use_qk_norm: True
  use_rope: True


SPEED BENCHMARKS
----------------------------------------------------------------------------------------------------
Batch    Seq Len    Forward      Backward     Total        FLOPS/s        
----------------------------------------------------------------------------------------------------
1        256        0.96         7.16         8.12         0.03           T
1        512        1.02         2.06         3.08         0.19           T
1        1024       1.10         2.38         3.48         0.45           T
4        256        0.98         2.35         3.33         0.29           T
4        512        1.02         2.01         3.03         0.77           T
4        1024       1.24         2.61         3.85         1.63           T
8        256        0.99         2.05         3.04         0.63           T
8        512        1.10         2.34         3.44         1.35           T
8        1024       1.44         3.35         4.79         2.61           T


MEMORY PROFILING
----------------------------------------------------------------------------------------------------
Batch    Seq Len    Peak MB      Activation MB   Per-Sample MB  
----------------------------------------------------------------------------------------------------
1        256        23.39        21.49           23.39          
1        512        30.03        28.13           30.03          
1        1024       39.14        37.24           39.14          
4        256        39.15        37.25           9.79           
4        512        57.85        55.95           14.46          
4        1024       94.28        92.38           23.57          
8        256        60.44        58.54           7.56           
8        512        94.29        92.40           11.79          
8        1024       166.89       164.99          20.86          


GRADIENT FLOW ANALYSIS
----------------------------------------------------------------------------------------------------
  Mean Gradient Norm: 1.98e+04
  Max Gradient Norm: 6.68e+04
  Min Gradient Norm: 1.13e+02
  Vanishing Gradients: False
  Exploding Gradients: True


LEARNING CAPABILITY
----------------------------------------------------------------------------------------------------
  Initial Loss: 1.0030
  Final Loss: 0.9981
  Loss Improvement: 0.49%
  Convergence Steps: 1
  Learning Capacity Score: 49.99


COMPONENT ANALYSIS
----------------------------------------------------------------------------------------------------

Compression Effectiveness:
  compression_factor: 4
  param_reduction_ratio: 0.75
  latent_dim: 192
  original_dim: 768
  convolutions_enabled: True

Head Reshaping Efficiency:
  n_heads: 12
  n_kv_heads: 3
  gqa_ratio: 4.0
  head_dim: 16
  kv_cache_reduction: 0.25


OPTIMIZATION RECOMMENDATIONS
----------------------------------------------------------------------------------------------------

1.   Backward pass is much slower than forward. Consider:
   - Enabling gradient checkpointing in full model
   - Using 16-bit precision (float16/bfloat16)
   - Reducing attention computation with lower compression

2.   Exploding gradients detected. Fix with:
   - Reduce learning rate
   - Enable gradient clipping
   - Use layer normalization before attention
   - Decrease compression factor (too aggressive compression)

3.   Low learning capacity score. Suggestions:
   - Reduce compression factor for better expressivity
   - Increase number of attention heads
   - Try different kernel sizes for convolutions
   - Verify data preprocessing is correct

====================================================================================================
TEST: 02_transformer_block
====================================================================================================

MODEL CONFIGURATION
----------------------------------------------------------------------------------------------------
  dim: None
  n_heads: None
  n_kv_heads: None
  compression_factor: None
  use_convs: None
  use_qk_mean: None
  use_qk_norm: None
  use_rope: None


SPEED BENCHMARKS
----------------------------------------------------------------------------------------------------
Batch    Seq Len    Forward      Backward     Total        FLOPS/s        
----------------------------------------------------------------------------------------------------
1        256        1.16         2.66         3.82         0.06           T
1        512        1.35         2.27         3.62         0.16           T
1        1024       1.54         2.67         4.22         0.37           T
4        256        1.45         2.35         3.80         0.25           T
4        512        1.63         2.69         4.32         0.54           T
4        1024       2.30         5.05         7.35         0.85           T
8        256        1.62         2.64         4.26         0.45           T
8        512        2.31         4.48         6.79         0.68           T
8        1024       3.46         8.55         12.01        1.04           T


MEMORY PROFILING
----------------------------------------------------------------------------------------------------
Batch    Seq Len    Peak MB      Activation MB   Per-Sample MB  
----------------------------------------------------------------------------------------------------
1        256        72.07        49.92           72.07          
1        512        107.52       85.37           107.52         
1        1024       144.86       122.70          144.86         
4        256        144.36       122.21          36.09          
4        512        226.02       203.87          56.51          
4        1024       387.11       364.95          96.78          
8        256        224.77       202.62          28.10          
8        512        387.11       364.96          48.39          
8        1024       711.79       689.64          88.97          


GRADIENT FLOW ANALYSIS
----------------------------------------------------------------------------------------------------
  Mean Gradient Norm: 4.02e+04
  Max Gradient Norm: 1.96e+05
  Min Gradient Norm: 2.77e+02
  Vanishing Gradients: False
  Exploding Gradients: True


LEARNING CAPABILITY
----------------------------------------------------------------------------------------------------
  Initial Loss: 2.0118
  Final Loss: 1.6839
  Loss Improvement: 16.30%
  Convergence Steps: 1
  Learning Capacity Score: 65.80


COMPONENT ANALYSIS
----------------------------------------------------------------------------------------------------

Compression Effectiveness:

Head Reshaping Efficiency:


OPTIMIZATION RECOMMENDATIONS
----------------------------------------------------------------------------------------------------

1.   Exploding gradients detected. Fix with:
   - Reduce learning rate
   - Enable gradient clipping
   - Use layer normalization before attention
   - Decrease compression factor (too aggressive compression)

====================================================================================================
TEST: 03_compression_factor_2x
====================================================================================================

MODEL CONFIGURATION
----------------------------------------------------------------------------------------------------
  dim: 768
  n_heads: 12
  n_kv_heads: 3
  compression_factor: 2
  use_convs: True
  use_qk_mean: True
  use_qk_norm: True
  use_rope: True


SPEED BENCHMARKS
----------------------------------------------------------------------------------------------------
Batch    Seq Len    Forward      Backward     Total        FLOPS/s        
----------------------------------------------------------------------------------------------------
1        256        0.89         2.10         2.98         0.16           T
1        512        1.04         2.07         3.11         0.37           T
4        256        1.19         2.14         3.32         0.58           T
4        512        1.13         2.13         3.25         1.43           T


MEMORY PROFILING
----------------------------------------------------------------------------------------------------
Batch    Seq Len    Peak MB      Activation MB   Per-Sample MB  
----------------------------------------------------------------------------------------------------
1        256        31.15        26.37           31.15          
1        512        40.05        35.27           40.05          
4        256        53.67        48.89           13.42          
4        512        80.88        76.10           20.22          


GRADIENT FLOW ANALYSIS
----------------------------------------------------------------------------------------------------
  Mean Gradient Norm: 1.17e+04
  Max Gradient Norm: 3.50e+04
  Min Gradient Norm: 1.02e+02
  Vanishing Gradients: False
  Exploding Gradients: True


LEARNING CAPABILITY
----------------------------------------------------------------------------------------------------
  Initial Loss: 1.0032
  Final Loss: 1.0016
  Loss Improvement: 0.16%
  Convergence Steps: 1
  Learning Capacity Score: 49.66


COMPONENT ANALYSIS
----------------------------------------------------------------------------------------------------

Compression Effectiveness:
  compression_factor: 2
  param_reduction_ratio: 0.5
  latent_dim: 384
  original_dim: 768
  convolutions_enabled: True

Head Reshaping Efficiency:
  n_heads: 12
  n_kv_heads: 3
  gqa_ratio: 4.0
  head_dim: 32
  kv_cache_reduction: 0.25


OPTIMIZATION RECOMMENDATIONS
----------------------------------------------------------------------------------------------------

1.   Backward pass is much slower than forward. Consider:
   - Enabling gradient checkpointing in full model
   - Using 16-bit precision (float16/bfloat16)
   - Reducing attention computation with lower compression

2.   Exploding gradients detected. Fix with:
   - Reduce learning rate
   - Enable gradient clipping
   - Use layer normalization before attention
   - Decrease compression factor (too aggressive compression)

3.   Low learning capacity score. Suggestions:
   - Reduce compression factor for better expressivity
   - Increase number of attention heads
   - Try different kernel sizes for convolutions
   - Verify data preprocessing is correct

====================================================================================================
TEST: 03_compression_factor_4x
====================================================================================================

MODEL CONFIGURATION
----------------------------------------------------------------------------------------------------
  dim: 768
  n_heads: 12
  n_kv_heads: 3
  compression_factor: 4
  use_convs: True
  use_qk_mean: True
  use_qk_norm: True
  use_rope: True


SPEED BENCHMARKS
----------------------------------------------------------------------------------------------------
Batch    Seq Len    Forward      Backward     Total        FLOPS/s        
----------------------------------------------------------------------------------------------------
1        256        0.83         1.98         2.81         0.09           T
1        512        1.00         1.81         2.81         0.21           T
4        256        1.08         1.76         2.84         0.34           T
4        512        1.01         1.78         2.79         0.83           T


MEMORY PROFILING
----------------------------------------------------------------------------------------------------
Batch    Seq Len    Peak MB      Activation MB   Per-Sample MB  
----------------------------------------------------------------------------------------------------
1        256        23.39        21.49           23.39          
1        512        30.03        28.13           30.03          
4        256        39.14        37.24           9.79           
4        512        57.85        55.95           14.46          


GRADIENT FLOW ANALYSIS
----------------------------------------------------------------------------------------------------
  Mean Gradient Norm: 9.04e+03
  Max Gradient Norm: 3.19e+04
  Min Gradient Norm: 1.50e+01
  Vanishing Gradients: False
  Exploding Gradients: True


LEARNING CAPABILITY
----------------------------------------------------------------------------------------------------
  Initial Loss: 1.0024
  Final Loss: 1.0012
  Loss Improvement: 0.12%
  Convergence Steps: 1
  Learning Capacity Score: 49.62


COMPONENT ANALYSIS
----------------------------------------------------------------------------------------------------

Compression Effectiveness:
  compression_factor: 4
  param_reduction_ratio: 0.75
  latent_dim: 192
  original_dim: 768
  convolutions_enabled: True

Head Reshaping Efficiency:
  n_heads: 12
  n_kv_heads: 3
  gqa_ratio: 4.0
  head_dim: 16
  kv_cache_reduction: 0.25


OPTIMIZATION RECOMMENDATIONS
----------------------------------------------------------------------------------------------------

1.   Exploding gradients detected. Fix with:
   - Reduce learning rate
   - Enable gradient clipping
   - Use layer normalization before attention
   - Decrease compression factor (too aggressive compression)

2.   Low learning capacity score. Suggestions:
   - Reduce compression factor for better expressivity
   - Increase number of attention heads
   - Try different kernel sizes for convolutions
   - Verify data preprocessing is correct

====================================================================================================
TEST: 03_compression_factor_8x
====================================================================================================

MODEL CONFIGURATION
----------------------------------------------------------------------------------------------------
  dim: 768
  n_heads: 12
  n_kv_heads: 3
  compression_factor: 8
  use_convs: True
  use_qk_mean: True
  use_qk_norm: True
  use_rope: True


SPEED BENCHMARKS
----------------------------------------------------------------------------------------------------
Batch    Seq Len    Forward      Backward     Total        FLOPS/s        
----------------------------------------------------------------------------------------------------
1        256        0.74         2.32         3.06         0.04           T
1        512        0.91         1.84         2.76         0.11           T
4        256        0.88         1.97         2.85         0.17           T
4        512        1.07         1.92         2.99         0.39           T


MEMORY PROFILING
----------------------------------------------------------------------------------------------------
Batch    Seq Len    Peak MB      Activation MB   Per-Sample MB  
----------------------------------------------------------------------------------------------------
1        256        20.85        20.03           20.85          
1        512        25.58        24.75           25.58          
4        256        32.43        31.61           8.11           
4        512        45.40        44.57           11.35          


GRADIENT FLOW ANALYSIS
----------------------------------------------------------------------------------------------------
  Mean Gradient Norm: 8.27e+03
  Max Gradient Norm: 2.91e+04
  Min Gradient Norm: 4.92e+01
  Vanishing Gradients: False
  Exploding Gradients: True


LEARNING CAPABILITY
----------------------------------------------------------------------------------------------------
  Initial Loss: 1.0028
  Final Loss: 1.0003
  Loss Improvement: 0.25%
  Convergence Steps: 1
  Learning Capacity Score: 49.75


COMPONENT ANALYSIS
----------------------------------------------------------------------------------------------------

Compression Effectiveness:
  compression_factor: 8
  param_reduction_ratio: 0.875
  latent_dim: 96
  original_dim: 768
  convolutions_enabled: True

Head Reshaping Efficiency:
  n_heads: 12
  n_kv_heads: 3
  gqa_ratio: 4.0
  head_dim: 8
  kv_cache_reduction: 0.25


OPTIMIZATION RECOMMENDATIONS
----------------------------------------------------------------------------------------------------

1.   Backward pass is much slower than forward. Consider:
   - Enabling gradient checkpointing in full model
   - Using 16-bit precision (float16/bfloat16)
   - Reducing attention computation with lower compression

2.   Exploding gradients detected. Fix with:
   - Reduce learning rate
   - Enable gradient clipping
   - Use layer normalization before attention
   - Decrease compression factor (too aggressive compression)

3.   Low learning capacity score. Suggestions:
   - Reduce compression factor for better expressivity
   - Increase number of attention heads
   - Try different kernel sizes for convolutions
   - Verify data preprocessing is correct

====================================================================================================
TEST: 04_hybrid_mqa
====================================================================================================

MODEL CONFIGURATION
----------------------------------------------------------------------------------------------------
  dim: 768
  n_heads: 12
  n_kv_heads: None
  compression_factor: None
  use_convs: None
  use_qk_mean: None
  use_qk_norm: None
  use_rope: True


SPEED BENCHMARKS
----------------------------------------------------------------------------------------------------
Batch    Seq Len    Forward      Backward     Total        FLOPS/s        
----------------------------------------------------------------------------------------------------
1        256        0.58         0.91         1.49         0.17           T
1        512        0.52         0.86         1.38         0.44           T
4        256        0.44         0.84         1.28         0.79           T
4        512        0.60         1.16         1.76         1.38           T


MEMORY PROFILING
----------------------------------------------------------------------------------------------------
Batch    Seq Len    Peak MB      Activation MB   Per-Sample MB  
----------------------------------------------------------------------------------------------------
1        256        30.79        25.91           30.79          
1        512        41.53        36.65           41.53          
4        256        57.06        52.18           14.27          
4        512        87.61        82.73           21.90          


GRADIENT FLOW ANALYSIS
----------------------------------------------------------------------------------------------------
  Mean Gradient Norm: 3.21e+05
  Max Gradient Norm: 7.80e+05
  Min Gradient Norm: 1.60e+04
  Vanishing Gradients: False
  Exploding Gradients: True


LEARNING CAPABILITY
----------------------------------------------------------------------------------------------------
  Initial Loss: 1.3341
  Final Loss: 1.0117
  Loss Improvement: 24.17%
  Convergence Steps: 3
  Learning Capacity Score: 72.67


COMPONENT ANALYSIS
----------------------------------------------------------------------------------------------------

Compression Effectiveness:

Head Reshaping Efficiency:
  n_heads: 12
  n_kv_heads: 12
  gqa_ratio: 1.0
  head_dim: 64
  kv_cache_reduction: 1.0


OPTIMIZATION RECOMMENDATIONS
----------------------------------------------------------------------------------------------------

1.   Exploding gradients detected. Fix with:
   - Reduce learning rate
   - Enable gradient clipping
   - Use layer normalization before attention
   - Decrease compression factor (too aggressive compression)

====================================================================================================
TEST: 04_hybrid_ccqa
====================================================================================================

MODEL CONFIGURATION
----------------------------------------------------------------------------------------------------
  dim: 768
  n_heads: 12
  n_kv_heads: 3
  compression_factor: 4
  use_convs: True
  use_qk_mean: True
  use_qk_norm: True
  use_rope: True


SPEED BENCHMARKS
----------------------------------------------------------------------------------------------------
Batch    Seq Len    Forward      Backward     Total        FLOPS/s        
----------------------------------------------------------------------------------------------------
1        256        1.13         2.85         3.98         0.06           T
1        512        1.22         2.70         3.92         0.15           T
4        256        1.31         2.73         4.04         0.24           T
4        512        1.31         2.73         4.04         0.57           T


MEMORY PROFILING
----------------------------------------------------------------------------------------------------
Batch    Seq Len    Peak MB      Activation MB   Per-Sample MB  
----------------------------------------------------------------------------------------------------
1        256        34.98        33.53           34.98          
1        512        41.86        40.41           41.86          
4        256        53.35        51.90           13.34          
4        512        77.40        75.95           19.35          


GRADIENT FLOW ANALYSIS
----------------------------------------------------------------------------------------------------
  Mean Gradient Norm: 8.30e+04
  Max Gradient Norm: 6.65e+05
  Min Gradient Norm: 1.17e+03
  Vanishing Gradients: False
  Exploding Gradients: True


LEARNING CAPABILITY
----------------------------------------------------------------------------------------------------
  Initial Loss: 1.3349
  Final Loss: 1.0242
  Loss Improvement: 23.28%
  Convergence Steps: 1
  Learning Capacity Score: 72.78


COMPONENT ANALYSIS
----------------------------------------------------------------------------------------------------

Compression Effectiveness:
  compression_factor: 4
  param_reduction_ratio: 0.75
  latent_dim: 192
  original_dim: 768
  convolutions_enabled: True

Head Reshaping Efficiency:
  n_heads: 12
  n_kv_heads: 3
  gqa_ratio: 4.0
  head_dim: 16
  kv_cache_reduction: 0.25


OPTIMIZATION RECOMMENDATIONS
----------------------------------------------------------------------------------------------------

1.   Backward pass is much slower than forward. Consider:
   - Enabling gradient checkpointing in full model
   - Using 16-bit precision (float16/bfloat16)
   - Reducing attention computation with lower compression

2.   Exploding gradients detected. Fix with:
   - Reduce learning rate
   - Enable gradient clipping
   - Use layer normalization before attention
   - Decrease compression factor (too aggressive compression)

====================================================================================================
TEST: 04_hybrid_mla
====================================================================================================

MODEL CONFIGURATION
----------------------------------------------------------------------------------------------------
  dim: 768
  n_heads: 12
  n_kv_heads: None
  compression_factor: None
  use_convs: None
  use_qk_mean: None
  use_qk_norm: None
  use_rope: True


SPEED BENCHMARKS
----------------------------------------------------------------------------------------------------
Batch    Seq Len    Forward      Backward     Total        FLOPS/s        
----------------------------------------------------------------------------------------------------
1        256        0.67         1.24         1.90         0.13           T
1        512        0.61         1.28         1.89         0.32           T
4        256        0.54         1.23         1.77         0.57           T
4        512        0.53         1.31         1.84         1.32           T


MEMORY PROFILING
----------------------------------------------------------------------------------------------------
Batch    Seq Len    Peak MB      Activation MB   Per-Sample MB  
----------------------------------------------------------------------------------------------------
1        256        42.50        38.56           42.50          
1        512        53.21        49.27           53.21          
4        256        67.99        64.05           17.00          
4        512        96.55        92.60           24.14          


GRADIENT FLOW ANALYSIS
----------------------------------------------------------------------------------------------------
  Mean Gradient Norm: 2.56e+05
  Max Gradient Norm: 8.66e+05
  Min Gradient Norm: 1.49e+04
  Vanishing Gradients: False
  Exploding Gradients: True


LEARNING CAPABILITY
----------------------------------------------------------------------------------------------------
  Initial Loss: 1.3278
  Final Loss: 1.0238
  Loss Improvement: 22.90%
  Convergence Steps: 1
  Learning Capacity Score: 72.40


COMPONENT ANALYSIS
----------------------------------------------------------------------------------------------------

Compression Effectiveness:

Head Reshaping Efficiency:
  n_heads: 12
  n_kv_heads: 12
  gqa_ratio: 1.0
  head_dim: 32
  kv_cache_reduction: 1.0


OPTIMIZATION RECOMMENDATIONS
----------------------------------------------------------------------------------------------------

1.   Backward pass is much slower than forward. Consider:
   - Enabling gradient checkpointing in full model
   - Using 16-bit precision (float16/bfloat16)
   - Reducing attention computation with lower compression

2.   Exploding gradients detected. Fix with:
   - Reduce learning rate
   - Enable gradient clipping
   - Use layer normalization before attention
   - Decrease compression factor (too aggressive compression)

====================================================================================================
TEST: 05_hybrid_macro_block
====================================================================================================

MODEL CONFIGURATION
----------------------------------------------------------------------------------------------------
  dim: None
  n_heads: None
  n_kv_heads: None
  compression_factor: None
  use_convs: None
  use_qk_mean: None
  use_qk_norm: None
  use_rope: None


SPEED BENCHMARKS
----------------------------------------------------------------------------------------------------
Batch    Seq Len    Forward      Backward     Total        FLOPS/s        
----------------------------------------------------------------------------------------------------
1        128        9.05         16.81        25.87        0.00           T
1        256        7.67         15.70        23.37        0.01           T
4        128        7.42         15.74        23.15        0.02           T
4        256        8.35         16.52        24.87        0.04           T


MEMORY PROFILING
----------------------------------------------------------------------------------------------------
Batch    Seq Len    Peak MB      Activation MB   Per-Sample MB  
----------------------------------------------------------------------------------------------------
1        128        473.93       249.00          473.93         
1        256        637.15       412.22          637.15         
4        128        797.31       572.39          199.33         
4        256        1119.49      894.56          279.87         


GRADIENT FLOW ANALYSIS
----------------------------------------------------------------------------------------------------
  Mean Gradient Norm: 1.12e+05
  Max Gradient Norm: 1.35e+06
  Min Gradient Norm: 1.66e+01
  Vanishing Gradients: False
  Exploding Gradients: True


LEARNING CAPABILITY
----------------------------------------------------------------------------------------------------
  Initial Loss: 3.5431
  Final Loss: 5.5700
  Loss Improvement: -57.21%
  Convergence Steps: 2
  Learning Capacity Score: -8.21


COMPONENT ANALYSIS
----------------------------------------------------------------------------------------------------

Compression Effectiveness:

Head Reshaping Efficiency:


OPTIMIZATION RECOMMENDATIONS
----------------------------------------------------------------------------------------------------

1.   Low compute throughput detected. Consider:
   - Using Flash Attention for sequence attention
   - Reducing kernel size for convolutions (from 3 to 1)
   - Decreasing sequence length sampling for training

2.   Backward pass is much slower than forward. Consider:
   - Enabling gradient checkpointing in full model
   - Using 16-bit precision (float16/bfloat16)
   - Reducing attention computation with lower compression

3.   High memory per sample. Consider:
   - Increasing compression factor (current may be too low)
   - Using mixed precision training (AMP)
   - Reducing model width or using grouped query attention

4.   Exploding gradients detected. Fix with:
   - Reduce learning rate
   - Enable gradient clipping
   - Use layer normalization before attention
   - Decrease compression factor (too aggressive compression)

5.   Low learning capacity score. Suggestions:
   - Reduce compression factor for better expressivity
   - Increase number of attention heads
   - Try different kernel sizes for convolutions
   - Verify data preprocessing is correct

====================================================================================================
TEST: 06_hybrid_transformer
====================================================================================================

MODEL CONFIGURATION
----------------------------------------------------------------------------------------------------
  dim: 768
  n_heads: 12
  n_kv_heads: 3
  compression_factor: 4
  use_convs: True
  use_qk_mean: True
  use_qk_norm: True
  use_rope: True


SPEED BENCHMARKS
----------------------------------------------------------------------------------------------------
Batch    Seq Len    Forward      Backward     Total        FLOPS/s        
----------------------------------------------------------------------------------------------------
1        64         22.04        48.63        70.67        0.00           T
1        128        21.76        48.19        69.95        0.00           T
2        64         21.77        47.93        69.70        0.00           T
2        128        21.84        47.87        69.71        0.00           T


MEMORY PROFILING
----------------------------------------------------------------------------------------------------
Batch    Seq Len    Peak MB      Activation MB   Per-Sample MB  
----------------------------------------------------------------------------------------------------
1        64         1193.54      626.76          1193.54        
1        128        1424.53      857.75          1424.53        
2        64         1424.55      857.77          712.28         
2        128        1622.84      1056.06         811.42         


GRADIENT FLOW ANALYSIS
----------------------------------------------------------------------------------------------------
  Mean Gradient Norm: 3.22e+04
  Max Gradient Norm: 6.78e+05
  Min Gradient Norm: 1.69e+01
  Vanishing Gradients: False
  Exploding Gradients: True


LEARNING CAPABILITY
----------------------------------------------------------------------------------------------------
  Initial Loss: 2.0007
  Final Loss: 1.7379
  Loss Improvement: 13.13%
  Convergence Steps: 1
  Learning Capacity Score: 62.63


COMPONENT ANALYSIS
----------------------------------------------------------------------------------------------------

Compression Effectiveness:
  compression_factor: 4
  param_reduction_ratio: 0.75
  latent_dim: 192
  original_dim: 768
  convolutions_enabled: True

Head Reshaping Efficiency:
  n_heads: 12
  n_kv_heads: 3
  gqa_ratio: 4.0
  head_dim: 64
  kv_cache_reduction: 0.25


OPTIMIZATION RECOMMENDATIONS
----------------------------------------------------------------------------------------------------

1.   Low compute throughput detected. Consider:
   - Using Flash Attention for sequence attention
   - Reducing kernel size for convolutions (from 3 to 1)
   - Decreasing sequence length sampling for training

2.   Backward pass is much slower than forward. Consider:
   - Enabling gradient checkpointing in full model
   - Using 16-bit precision (float16/bfloat16)
   - Reducing attention computation with lower compression

3.   High memory per sample. Consider:
   - Increasing compression factor (current may be too low)
   - Using mixed precision training (AMP)
   - Reducing model width or using grouped query attention

4.   Exploding gradients detected. Fix with:
   - Reduce learning rate
   - Enable gradient clipping
   - Use layer normalization before attention
   - Decrease compression factor (too aggressive compression)
