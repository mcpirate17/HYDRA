{
  "timestamp": "2025-12-07 14:46:18",
  "model_config": {
    "dim": 768,
    "n_heads": 12,
    "n_kv_heads": null,
    "compression_factor": null,
    "use_convs": null,
    "use_qk_mean": null,
    "use_qk_norm": null,
    "use_rope": true
  },
  "speed_results": [
    {
      "forward_time_ms": 0.6682600011117756,
      "backward_time_ms": 1.2361399974906817,
      "total_time_ms": 1.9043999986024573,
      "throughput_samples_per_sec": 525.0997693414457,
      "batch_size": 1,
      "seq_len": 256,
      "flops": 252444672,
      "flops_per_sec": 132558639038.67693
    },
    {
      "forward_time_ms": 0.6104599975515157,
      "backward_time_ms": 1.2809400010155514,
      "total_time_ms": 1.891399998567067,
      "throughput_samples_per_sec": 528.7088932841305,
      "batch_size": 1,
      "seq_len": 512,
      "flops": 605552640,
      "flops_per_sec": 320161066119.6835
    },
    {
      "forward_time_ms": 0.5389399942941964,
      "backward_time_ms": 1.233900006627664,
      "total_time_ms": 1.7728400009218603,
      "throughput_samples_per_sec": 2256.2667798109455,
      "batch_size": 4,
      "seq_len": 256,
      "flops": 1009778688,
      "flops_per_sec": 569582527173.8704
    },
    {
      "forward_time_ms": 0.5310100095812231,
      "backward_time_ms": 1.3077900017378852,
      "total_time_ms": 1.8388000113191083,
      "throughput_samples_per_sec": 2175.331724699361,
      "batch_size": 4,
      "seq_len": 512,
      "flops": 2422210560,
      "flops_per_sec": 1317277868767.4512
    }
  ],
  "memory_results": [
    {
      "peak_memory_mb": 42.5009765625,
      "activation_memory_mb": 38.55908203125,
      "parameter_memory_mb": 3.94189453125,
      "allocated_memory_mb": 40.05224609375,
      "reserved_memory_mb": 54.0,
      "batch_size": 1,
      "seq_len": 256,
      "memory_per_sample_mb": 42.5009765625
    },
    {
      "peak_memory_mb": 53.20703125,
      "activation_memory_mb": 49.26513671875,
      "parameter_memory_mb": 3.94189453125,
      "allocated_memory_mb": 41.55224609375,
      "reserved_memory_mb": 64.0,
      "batch_size": 1,
      "seq_len": 512,
      "memory_per_sample_mb": 53.20703125
    },
    {
      "peak_memory_mb": 67.98828125,
      "activation_memory_mb": 64.04638671875,
      "parameter_memory_mb": 3.94189453125,
      "allocated_memory_mb": 44.55224609375,
      "reserved_memory_mb": 74.0,
      "batch_size": 4,
      "seq_len": 256,
      "memory_per_sample_mb": 16.9970703125
    },
    {
      "peak_memory_mb": 96.546875,
      "activation_memory_mb": 92.60498046875,
      "parameter_memory_mb": 3.94189453125,
      "allocated_memory_mb": 50.55224609375,
      "reserved_memory_mb": 112.0,
      "batch_size": 4,
      "seq_len": 512,
      "memory_per_sample_mb": 24.13671875
    }
  ],
  "gradient_analysis": {
    "mean_grad_norm": 256248.38220214844,
    "max_grad_norm": 866010.8125,
    "min_grad_norm": 14861.501953125,
    "grad_norm_q": 15224.99609375,
    "grad_norm_k": 19201.78515625,
    "grad_norm_v": 576747.8125,
    "grad_norm_o": 526773.8125,
    "vanishing_gradient_detected": false,
    "exploding_gradient_detected": true
  },
  "learning_results": {
    "initial_loss": 1.3277562856674194,
    "final_loss": 1.0237516164779663,
    "loss_improvement": 0.2289611958018546,
    "convergence_steps": 1,
    "gradient_flow_score": 2.185222847469182,
    "learning_capacity_score": 72.39611958018546
  },
  "component_analysis": {
    "compression_effectiveness": {},
    "qk_mean_impact": {},
    "head_reshaping_efficiency": {
      "n_heads": 12,
      "n_kv_heads": 12,
      "gqa_ratio": 1.0,
      "head_dim": 32,
      "kv_cache_reduction": 1.0
    },
    "normalization_stats": {}
  },
  "optimization_recommendations": [
    "\u26a0\ufe0f  Backward pass is much slower than forward. Consider:\n   - Enabling gradient checkpointing in full model\n   - Using 16-bit precision (float16/bfloat16)\n   - Reducing attention computation with lower compression",
    "\u26a0\ufe0f  Exploding gradients detected. Fix with:\n   - Reduce learning rate\n   - Enable gradient clipping\n   - Use layer normalization before attention\n   - Decrease compression factor (too aggressive compression)"
  ]
}