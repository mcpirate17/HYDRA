# HYDRA: Hybrid Dynamic Routing Architecture

<p align="center">
  <img src="docs/hydra_architecture.png" alt="HYDRA Architecture" width="600">
</p>

> **A scalable transformer architecture combining Compressed Convolutional Grouped Query Attention (CCGQA), Mixture-of-Depths (MoD), and Mixture-of-Recursions (MoR) for efficient and adaptive language modeling.**

---

## ğŸ¯ Overview

HYDRA is a modern transformer architecture that achieves **state-of-the-art efficiency** through three synergistic innovations:

| Component | Paper | Key Innovation |
|-----------|-------|----------------|
| **CCGQA** | [arXiv:2510.04476](https://arxiv.org/abs/2510.04476) | Attention in compressed latent space with convolutions |
| **MoD** | [arXiv:2404.02258](https://arxiv.org/abs/2404.02258) | Token-level dynamic computation routing |
| **MoR** | [arXiv:2507.10524](https://arxiv.org/abs/2507.10524) | Layer-level adaptive depth with recursion |

### Why "HYDRA"?

Like the mythical multi-headed Hydra, this architecture features **multiple routing heads** that dynamically adapt computation:
- **MoD heads** decide which tokens need full processing
- **MoR heads** decide how many recursive layers each position needs
- **CCGQA heads** perform efficient compressed attention

---

## ğŸ—ï¸ Architecture

### High-Level Structure

```
Input Tokens
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Token Embedding                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MoD Router (Token Selection)                   â”‚
â”‚     "Which tokens need full computation this layer?"        â”‚
â”‚     - Soft routing during training (all tokens, weighted)   â”‚
â”‚     - Hard top-k routing during inference (75% capacity)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   MoR Block (Recursive)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              MoR Router (Depth Selection)             â”‚  â”‚
â”‚  â”‚  "How many recursive iterations for this position?"   â”‚  â”‚
â”‚  â”‚  - Gaussian soft routing during training              â”‚  â”‚
â”‚  â”‚  - Layer-aware: early layers 40%, late layers 80%     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â”‚                                   â”‚
â”‚                          â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              CCGQA Attention Block                    â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  Input â”€â”€â–º Compress (4x) â”€â”€â–º Q,K,V Projections       â”‚  â”‚
â”‚  â”‚                â”‚                                      â”‚  â”‚
â”‚  â”‚                â–¼                                      â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚
â”‚  â”‚  â”‚ Sequence Conv â”€â”€â–º Channel Conv â”€â”€â–º QK Mean   â”‚    â”‚  â”‚
â”‚  â”‚  â”‚ (causal, k=3)    (pointwise)     (coupling)  â”‚    â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚
â”‚  â”‚                â”‚                                      â”‚  â”‚
â”‚  â”‚                â–¼                                      â”‚  â”‚
â”‚  â”‚  QK L2 Norm + Temperature â”€â”€â–º Attention â”€â”€â–º Value    â”‚  â”‚
â”‚  â”‚                â”‚                                      â”‚  â”‚
â”‚  â”‚                â–¼                                      â”‚  â”‚
â”‚  â”‚  Expand (4x) â”€â”€â–º Residual Add â”€â”€â–º FFN â”€â”€â–º Output     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â”‚                                   â”‚
â”‚                    (Repeat Ã— r recursions)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼ (Repeat Ã— n_blocks with MoD routing)
     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Final LayerNorm                          â”‚
â”‚                    LM Head â†’ Logits                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Model Variants

HYDRA supports multiple scales with validated compliance:

| Variant | Parameters | Dim | Layers | MoR Blocks Ã— Recursions | Status |
|---------|------------|-----|--------|-------------------------|--------|
| **100M** | ~100M | 768 | 32 | 8 Ã— 4 | âœ… Validated |
| **250M** | ~216M | 1024 | 48 | 12 Ã— 4 | âœ… Validated |
| **500M** | ~570M | 1536 | 64 | 16 Ã— 4 | âœ… Validated |
| **750M** | ~927M | 1792 | 80 | 20 Ã— 4 | âœ… Validated |
| **900M** | ~1.2B | 2048 | 80 | 20 Ã— 4 | âœ… Validated |
| **1B** | ~1.4B | 2048 | 96 | 24 Ã— 4 | âœ… Validated |
| **1.5B** | ~2.2B | 2560 | 120 | 24 Ã— 5 | âœ… Validated |
| **4B** | ~4B | 4096 | 160 | 40 Ã— 4 | ğŸ“ˆ Predicted |

---

## ğŸ”¬ Paper Compliance

### CCGQA (arXiv:2510.04476)

| Requirement | Implementation | Status |
|-------------|----------------|--------|
| 4Ã— compression factor | `compression_factor=4` | âœ… |
| Sequence convolutions | Causal 1D conv, kernel=3 | âœ… |
| Channel convolutions | Pointwise 1Ã—1 conv | âœ… |
| QK-mean coupling | Mean shared before/after conv | âœ… |
| QK L2 normalization | With learnable temperature | âœ… |
| GQA head sharing | `n_kv_heads < n_heads` | âœ… |
| Value shift | Half heads see previous token | âœ… |

### MoD (arXiv:2404.02258)

| Requirement | Implementation | Status |
|-------------|----------------|--------|
| Router architecture | Linear projection + sigmoid | âœ… |
| Soft routing (training) | Weighted sum by router probs | âœ… |
| Hard routing (inference) | Top-k selection, k=capacity | âœ… |
| 75% capacity target | `capacity_ratio=0.75` | âœ… |
| Auxiliary loss | BCE to maintain capacity | âœ… |
| Auto-scaling aux weight | `0.01 * (L/32) * âˆš(d/768)` | âœ… |

### MoR (arXiv:2507.10524)

| Requirement | Implementation | Status |
|-------------|----------------|--------|
| Gaussian soft routing | `N(Î¼, Ïƒ)` weighting over depths | âœ… |
| Layer-aware capacity | Early: 40%, Late: 80% | âœ… |
| Recursion embeddings | Additive depth embeddings | âœ… |
| Ponder loss | Encourages early stopping | âœ… |
| Depth histogram | Per-layer depth distribution | âœ… |

---

## ğŸ“ Project Structure

```
HYDRA/
â”œâ”€â”€ hydra/                    # Main package
â”‚   â”œâ”€â”€ __init__.py          # Package exports
â”‚   â”œâ”€â”€ model/               # Core model components
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ ccgqa.py         # CCGQA + MoD + MoR implementation
â”‚   â””â”€â”€ routing/             # Routing modules (MoD, MoR)
â”‚       â””â”€â”€ __init__.py
â”œâ”€â”€ tests/                   # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_paper_compliance.py  # 64 compliance tests
â”œâ”€â”€ diagnostics/             # Scaling and compliance tools
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ scaling_analysis.py  # Multi-scale curve fitting
â”‚   â”œâ”€â”€ run_variant_diagnostics.py
â”‚   â””â”€â”€ deep_diagnosis.py
â”œâ”€â”€ configs/                 # Model configurations
â”‚   â””â”€â”€ variants.yaml        # Model variant definitions
â”œâ”€â”€ reports/                 # Generated analysis reports
â”‚   â”œâ”€â”€ scaling_analysis.png
â”‚   â”œâ”€â”€ scaling_summary_table.png
â”‚   â””â”€â”€ scaling_analysis_results.json
â”œâ”€â”€ docs/                    # Documentation
â”‚   â””â”€â”€ ARCHITECTURE.md      # This file
â”œâ”€â”€ README.md               # Project overview
â”œâ”€â”€ pytest.ini              # Test configuration
â””â”€â”€ requirements.txt        # Dependencies
```

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourname/hydra.git
cd hydra

# Install dependencies
pip install -r requirements.txt

# Install in development mode
pip install -e .
```

### Basic Usage

```python
from hydra import create_ccgqa_mod_mor_model

# Create a 100M model
model = create_ccgqa_mod_mor_model(
    vocab_size=32000,
    dim=768,
    n_mor_blocks=8,
    recursions=4,
    n_heads=12,
    n_kv_heads=3,
    compression_factor=4,
    capacity_ratio=0.75,
)

# Forward pass
input_ids = torch.randint(0, 32000, (1, 512))
outputs = model(input_ids)

logits = outputs["logits"]           # [batch, seq, vocab]
aux_loss = outputs["aux_loss"]       # MoD capacity loss
ponder_loss = outputs["ponder_loss"] # MoR depth loss

# Total loss for training
total_loss = ce_loss + 0.01 * aux_loss + 0.01 * ponder_loss
```

### Run Tests

```bash
# Run full compliance test suite (64 tests)
pytest tests/test_paper_compliance.py -v

# Run fast tests only
pytest tests/test_paper_compliance.py -v -m "not slow"
```

### Run Diagnostics

```bash
# Run scaling analysis across all variants
python diagnostics/scaling_analysis.py \
    --variants 100M 250M 500M 750M 900M 1B 1.5B \
    --steps 30 \
    --plot \
    --predict-4b \
    --output reports/scaling_analysis_results.json
```

---

## ğŸ“ˆ Scaling Analysis

The architecture has been validated across 7 model scales with curve fitting to predict 4B behavior:

### Curve Fitting Results

| Metric | Best Fit | RÂ² | 4B Prediction |
|--------|----------|-----|---------------|
| `aux_loss_weight` | Polynomial (deg 2) | 0.990 | ~0.102 |
| `mod_prob` | Polynomial (deg 2) | 0.901 | Stable ~0.75 |
| `mor_depth` | Constant | 1.000 | 1.0 |
| Compute time | Polynomial (deg 2) | 0.951 | ~47s/step |

### Auto-Scaling Formula

For large models, the auxiliary loss weight automatically scales:

```python
aux_loss_weight = 0.01 * (effective_layers / 32) * sqrt(dim / 768)
```

This ensures MoD capacity remains at 75% even as model size increases.

---

## ğŸ§ª Testing Philosophy

HYDRA follows a rigorous testing philosophy:

1. **Paper Compliance**: Every architectural claim is validated against the source papers
2. **Scale Invariance**: Tests run at multiple scales (100M â†’ 1.5B)
3. **Repeatability**: All tests use fixed seeds and are deterministic
4. **Regression Prevention**: Scaling analysis detects drift in hyperparameters

---

## ğŸ“š References

```bibtex
@article{ccgqa2024,
  title={Compressed Convolutional Attention: Efficient Attention in a Compressed Latent Space},
  author={...},
  journal={arXiv preprint arXiv:2510.04476},
  year={2024}
}

@article{mod2024,
  title={Mixture-of-Depths: Dynamically allocating compute in transformer-based language models},
  author={Raposo, David and others},
  journal={arXiv preprint arXiv:2404.02258},
  year={2024}
}

@article{mor2025,
  title={Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation},
  author={...},
  journal={arXiv preprint arXiv:2507.10524},
  year={2025}
}
```

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) for details.

---

## ğŸ¤ Contributing

Contributions welcome! Please read [CONTRIBUTING.md](docs/CONTRIBUTING.md) first.

---

<p align="center">
  <strong>HYDRA</strong> - Multi-headed efficiency for modern transformers ğŸ‰
</p>
