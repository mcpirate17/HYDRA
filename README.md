# HYDRA: Hybrid Dynamic Routing Architecture

> **A scalable transformer architecture combining Compressed Convolutional Grouped Query Attention (CCGQA), Mixture-of-Depths (MoD), and Mixture-of-Recursions (MoR) for efficient and adaptive language modeling.**

---

## ğŸ¯ Overview

HYDRA is a modern transformer architecture that achieves **state-of-the-art efficiency** through three synergistic innovations:

| Component | Paper | Key Innovation |
|-----------|-------|----------------|
| **CCGQA** | [arXiv:2510.04476](https://arxiv.org/abs/2510.04476) | Attention in compressed latent space with convolutions |
| **MoD** | [arXiv:2404.02258](https://arxiv.org/abs/2404.02258) | Token-level dynamic computation routing |
| **MoR** | [arXiv:2507.10524](https://arxiv.org/abs/2507.10524) | Layer-level adaptive depth with recursion |

### Why "HYDRA"?

Like the mythical multi-headed Hydra, this architecture features **multiple routing heads** that dynamically adapt computation:
- **MoD heads** decide which tokens need full processing
- **MoR heads** decide how many recursive layers each position needs
- **CCGQA heads** perform efficient compressed attention

---

## ğŸ—ï¸ Architecture

### High-Level Structure

```
Input Tokens
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Token Embedding                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MoD Router (Token Selection)                   â”‚
â”‚     "Which tokens need full computation this layer?"        â”‚
â”‚     - Soft routing during training (all tokens, weighted)   â”‚
â”‚     - Hard top-k routing during inference (75% capacity)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   MoR Block (Recursive)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              MoR Router (Depth Selection)             â”‚  â”‚
â”‚  â”‚  "How many recursive iterations for this position?"   â”‚  â”‚
â”‚  â”‚  - Gaussian soft routing during training              â”‚  â”‚
â”‚  â”‚  - Layer-aware: early layers 40%, late layers 80%     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â”‚                                   â”‚
â”‚                          â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              CCGQA Attention Block                    â”‚  â”‚
â”‚  â”‚                                                       â”‚  â”‚
â”‚  â”‚  Input â”€â”€â–º Compress (4x) â”€â”€â–º Q,K,V Projections       â”‚  â”‚
â”‚  â”‚                â”‚                                      â”‚  â”‚
â”‚  â”‚                â–¼                                      â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚
â”‚  â”‚  â”‚ Sequence Conv â”€â”€â–º Channel Conv â”€â”€â–º QK Mean   â”‚    â”‚  â”‚
â”‚  â”‚  â”‚ (causal, k=3)    (pointwise)     (coupling)  â”‚    â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚
â”‚  â”‚                â”‚                                      â”‚  â”‚
â”‚  â”‚                â–¼                                      â”‚  â”‚
â”‚  â”‚  QK L2 Norm + Temperature â”€â”€â–º Attention â”€â”€â–º Value    â”‚  â”‚
â”‚  â”‚                â”‚                                      â”‚  â”‚
â”‚  â”‚                â–¼                                      â”‚  â”‚
â”‚  â”‚  Expand (4x) â”€â”€â–º Residual Add â”€â”€â–º FFN â”€â”€â–º Output     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                          â”‚                                   â”‚
â”‚                    (Repeat Ã— r recursions)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼ (Repeat Ã— n_blocks with MoD routing)
     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Final LayerNorm                          â”‚
â”‚                    LM Head â†’ Logits                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Model Variants

HYDRA supports multiple scales optimized for different GPU memory budgets:

| Variant | Parameters | Dim | MoR Blocks Ã— Rec | Eff Layers | GPU Memory | Status |
|---------|------------|-----|------------------|------------|------------|--------|
| **debug** | ~33M | 512 | 2 Ã— 2 | 4 | ~4GB | âœ… Fast iteration |
| **50M** | ~50M | 512 | 8 Ã— 3 | 24 | ~8GB | âœ… Deep & narrow |
| **100M** | ~104M | 768 | 8 Ã— 4 | 32 | ~14GB | âœ… Validated |
| **250M** | ~198M | 1024 | 10 Ã— 4 | 40 | ~18GB | âœ… Validated |
| **500M** | ~426M | 1280 | 16 Ã— 4 | 64 | ~22GB | âœ… Validated |
| **750M** | ~665M | 1536 | 18 Ã— 4 | 72 | ~26GB | âœ… Validated |
| **1B** | ~973M | 1792 | 20 Ã— 4 | 80 | ~29GB | âœ… Validated |
| **1.5B** | ~1,369M | 2048 | 22 Ã— 4 | 88 | ~36GB | âš ï¸ 48GB+ GPU |

> **Note:** GPU memory is peak usage during training with 8-bit Adam + gradient checkpointing on RTX 5090 32GB.
>
> **50M "deep" config:** Designed for MoD/MoR curriculum validation. Narrow (dim=512) but deep (24 effective layers) to test dynamic routing effectiveness.

---

## ğŸ”¬ Attention Architecture: CCGQA

HYDRA uses **Compressed Convolutional Grouped Query Attention (CCGQA)** exclusively. CCGQA achieves superior convergence and memory efficiency through:

- **Compression**: 4Ã— dimensionality reduction before attention computation
- **Convolution**: Causal sequence and channel convolutions for efficient feature extraction
- **Grouped Query Attention**: Head sharing (4:1 to 8:1 GQA ratio) reduces KV cache memory
- **Coupled QK Normalization**: Shared attention statistics improve training stability

### CCGQA Performance Summary

**Recent Training Results (December 2024-2025):**

| Model Size | Final Loss | Best Loss | Convergence | Throughput | Memory |
|------------|-----------|----------|------------|-----------|--------|
| **100M** | 3.81 | 3.75 | âœ… Fast | 30K tok/s | 14GB |
| **250M** | 3.21 | 3.18 | âœ… Good | 20K tok/s | 18GB |
| **500M** | 2.92 | 2.88 | âœ… Good | 12K tok/s | 22GB |
| **1B** | 2.48 | 2.44 | âœ… Steady | 5K tok/s | 29GB |

**Architecture Highlights:**

| Component | Specification | Benefit |
|-----------|---------------|---------|
| **Compression** | 4Ã— latent space | 16Ã— fewer attention ops |
| **Convolutions** | Causal seq (k=3) + pointwise | Efficient pattern extraction |
| **GQA Ratio** | 4:1 to 8:1 KV sharing | Reduced memory footprint |
| **QK Norm** | L2 norm + learned temperature | Stable gradients |
| **Value Shift** | Half heads see t-1 | Better information flow |

```bash
# Train with CCGQA attention (all models use this exclusively)
python trainer.py --model_size 100M --max_steps 5000
python trainer.py --model_size 500M --max_steps 10000
python trainer.py --model_size 1B --max_steps 20000
```

### Block Architecture

Each **MoR Block** contains the following layers:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MoR Block (repeated n_mor_blocks times)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. CCGQA Attention                                             â”‚
â”‚     â”œâ”€â”€ RMSNorm (pre-norm)                                      â”‚
â”‚     â”œâ”€â”€ Q/K/V Linear projections (dim â†’ n_heads Ã— head_dim)    â”‚
â”‚     â”œâ”€â”€ RoPE positional embeddings                              â”‚
â”‚     â”œâ”€â”€ Grouped Query Attention (4:1 to 8:1 GQA ratio)         â”‚
â”‚     â”œâ”€â”€ Context Compression (for long sequences)                â”‚
â”‚     â””â”€â”€ Output Linear projection                                â”‚
â”‚                                                                 â”‚
â”‚  2. SwiGLU MLP                                                  â”‚
â”‚     â”œâ”€â”€ RMSNorm (pre-norm)                                      â”‚
â”‚     â”œâ”€â”€ Gate Linear (dim â†’ hidden_dim)                         â”‚
â”‚     â”œâ”€â”€ Up Linear (dim â†’ hidden_dim)                           â”‚
â”‚     â”œâ”€â”€ SiLU activation Ã— gate                                  â”‚
â”‚     â””â”€â”€ Down Linear (hidden_dim â†’ dim)                         â”‚
â”‚                                                                 â”‚
â”‚  3. MoD Router (Mixture of Depths)                              â”‚
â”‚     â””â”€â”€ Token-level routing (75% capacity, skip unimportant)    â”‚
â”‚                                                                 â”‚
â”‚  4. MoR Router (Mixture of Recursions)                          â”‚
â”‚     â”œâ”€â”€ Recursion embedding (one per recursion depth)           â”‚
â”‚     â””â”€â”€ Decides which tokens need more processing               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Effective Layers** = `n_mor_blocks Ã— recursions` (weights are shared across recursions within each block)

### Training Metrics & Performance (Validated December 2024-January 2025)

**Key Observations from Production Runs:**

| Metric | 100M | 250M | 500M | 1B |
|--------|------|------|------|-----|
| **Convergence Speed** | 3.5K steps | 5K steps | 8K steps | 12K steps |
| **Final Loss** | 3.81 | 3.21 | 2.92 | 2.48 |
| **Training Efficiency** | âœ… Excellent | âœ… Good | âœ… Good | âœ… Steady |
| **Tokens/Second** | 30K | 20K | 12K | 5K |
| **Peak Memory** | 14GB | 18GB | 22GB | 29GB |
| **Effective Layers** | 32 | 40 | 56 | 80 |
| **GQA Ratio** | 4:1 | 4:1 | 7:1 | 8:1 |

**Routing Dynamics (MoD & MoR):**

- **MoD Activation**: Enables at ~10% of training when CE loss < 5.0
  - Results in ~50% compute savings after full activation
  - Learns to skip easy tokens while preserving learning capacity
  
- **MoR Adaptive Depth**: Enables at ~20% of training
  - Early layers: ~40% tokens use shallow recursion
  - Late layers: ~80% tokens use deep recursion
  - Reduces overall FLOPs without sacrificing convergence

---

## ğŸš€ Training on RTX 5090 (32GB)

### Memory Requirements by Model Size

Benchmarked on RTX 5090 32GB with 8-bit Adam + gradient checkpointing (every layer):

| Model | Actual Params | Dim | Blocks Ã— Rec | Eff Layers | Batch | Accum | Peak Mem | Throughput |
|-------|---------------|-----|--------------|------------|-------|-------|----------|------------|
| **100M** | ~104M | 768 | 8 Ã— 4 | 32 | 32 | 4 | ~14GB | ~30K tok/s |
| **250M** | ~198M | 1024 | 10 Ã— 4 | 40 | 24 | 5 | ~18GB | ~20K tok/s |
| **500M** | ~426M | 1280 | 16 Ã— 4 | 64 | 8 | 8 | ~22GB | ~12K tok/s |
| **750M** | ~665M | 1536 | 18 Ã— 4 | 72 | 4 | 16 | ~26GB | ~8K tok/s |
| **1B** | ~973M | 1792 | 20 Ã— 4 | 80 | 2 | 30 | ~29GB | ~5K tok/s |
| **1.5B** | ~1,369M | 2048 | 22 Ã— 4 | 88 | 1 | 60 | ~36GB | âš ï¸ 48GB+ |

> âš ï¸ **1B Model Warning:** `batch_size=3` peaks at ~32GB (borderline on 32GB GPU), `batch_size=4+` will OOM!
> 
> âš ï¸ **1.5B Model:** Requires 48GB+ VRAM (A6000, RTX 6000, or multi-GPU setup)

### Required Flags for Large Models (750M+)

```bash
--8bit_adam              # Essential - saves ~75% optimizer memory
--checkpoint_every 1     # Gradient checkpointing on every layer
```

### Optimizer Options

| Optimizer | Optimizer State Memory* | Speed | Stability | CLI Flag | Notes |
|-----------|-------------------------|-------|-----------|----------|-------|
| **Fused AdamW** (default) | 100% (2Ã— params) | Fast | Stable | _(default)_ | PyTorch native, battle-tested |
| **8-bit Adam** | **25%** (0.5Ã— params) | Fast | Stable | `--8bit_adam` | Essential for 1B+. Requires bitsandbytes |
| **Adafactor** | **<25%** (adaptive) | Medium | Good | `--adafactor` | No momentum state. Internal 1/âˆšt schedule |
| **Lion** | **50%** (1Ã— params) | Fastest | Good | _(not wired)_ | 3-10x lower LR than AdamW. Needs higher WD |
| **C-Lion** | **50%** (1Ã— params) | Fastest | Better | _(not wired)_ | Cautious variant, only updates when signs agree |
| **Muon** | 100% (2Ã— params) | Slow | Research | _(not wired)_ | Newton-Schulz orthogonalization. 2D params only |
| **Sophia-G** | 100% (2Ã— params) | Slow | Research | _(not wired)_ | Hessian-based. Expensive Hessian estimation |

> \* **Optimizer state only** (momentum + variance buffers), not total VRAM. Total VRAM = weights + gradients + optimizer state + activations.
> 
> Example (500M params, bfloat16):
> - Weights: 1GB, Gradients: 1GB, AdamW state: 4GB â†’ **8-bit Adam state: 1GB** (saves 3GB total)
> 
> ğŸ”¬ **Research optimizers** (Lion, C-Lion, Muon, Sophia) are implemented in `hydra/optim/` but not yet CLI-accessible. To use them, you'll need to modify `hydra/training/trainer.py` `_setup_optimizer()` method.

### Training Commands

**100M Model (quick testing):**
```bash
python trainer.py \
  --model_size 100M \
  --mode testing \
  --max_steps 1000
```

**1B Model (production):**
```bash
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True python trainer.py \
    --model_size 1B \
    --mode production \
    --8bit_adam \
    --checkpoint_every 1 \
    --adaptive_lr \
    --triton_kernels \
    --chunked_ce \
    --dataset finefineweb-sequential \
    --seed 42
```

> **Note:** Batch size and gradient accumulation are automatically set based on `--model_size`. Override with `--batch_size` and `--grad_accum` if needed.

### Sequence Length (1024/2048) â€” RTX 5090

**Recommended settings** (torch.compile, Triton, bfloat16 AMP, gradient checkpointing, chunked CE size 4096, 8-bit Adam required). Use `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True` to reduce fragmentation.

> âš ï¸ **System Overhead:** Leave 2-3GB headroom for display server + CUDA runtime. Batch sizes below are **trainer defaults** (auto-selected when you specify `--model_size` + `--8bit_adam`).

| Model | seq_len | batch_size | grad_accum | Expected VRAM | Throughput |
|-------|---------|------------|------------|---------------|------------|
| 500M  | 1024    | 4          | 15         | ~22-24GB      | ~6.7K tok/s |
| 500M  | 2048    | 4          | 15         | ~27-29GB      | ~7.0K tok/s |
| 750M  | 1024    | 4          | 16         | ~26-28GB      | ~6.4K tok/s |
| 750M  | 2048    | 4          | 16         | ~29-31GB âš ï¸   | ~6.9K tok/s |
| 1B    | 1024    | 2          | 30         | ~19-21GB      | ~4.6K tok/s |
| 1B    | 2048    | 2          | 30         | ~26-28GB      | ~4.8K tok/s |

> ğŸ’¡ **Auto-tuning:** The trainer automatically selects `batch_size` and `grad_accum` from [MODEL_SIZE_CONFIGS](hydra/training/config.py#L350). Override with `--batch_size` / `--grad_accum` only if you need different throughput/memory trade-offs.

Examples:

```bash
# 500M @ 1024 (trainer defaults: bs=4, accum=15)
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True python trainer.py \
  --model_size 500M \
  --mode production \
  --seq_len 1024 \
  --compile \
  --gradient_checkpointing \
  --triton_kernels \
  --chunked_ce \
  --chunked_ce_size 4096 \
  --8bit_adam \
  --dataset finefineweb-sequential

# 500M @ 2048 (trainer defaults: bs=4, accum=15)
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True python trainer.py \
  --model_size 500M \
  --mode production \
  --seq_len 2048 \
  --compile \
  --gradient_checkpointing \
  --triton_kernels \
  --chunked_ce \
  --chunked_ce_size 4096 \
  --8bit_adam \
  --dataset finefineweb-sequential

# 750M @ 1024 (trainer defaults: bs=4, accum=16)
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True python trainer.py \
  --model_size 750M \
  --mode production \
  --seq_len 1024 \
  --compile \
  --gradient_checkpointing \
  --triton_kernels \
  --chunked_ce \
  --chunked_ce_size 4096 \
  --8bit_adam \
  --dataset finefineweb-sequential

# 750M @ 2048 (trainer defaults: bs=4, accum=16 â€” tight fit!)
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True python trainer.py \
  --model_size 750M \
  --mode production \
  --seq_len 2048 \
  --compile \
  --gradient_checkpointing \
  --triton_kernels \
  --chunked_ce \
  --chunked_ce_size 4096 \
  --8bit_adam \
  --dataset finefineweb-sequential

# 1B @ 1024 (trainer defaults: bs=2, accum=30)
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True python trainer.py \
  --model_size 1B \
  --mode production \
  --seq_len 1024 \
  --compile \
  --gradient_checkpointing \
  --triton_kernels \
  --chunked_ce \
  --chunked_ce_size 4096 \
  --8bit_adam \
  --dataset finefineweb-sequential

# 1B @ 2048 (trainer defaults: bs=2, accum=30)
PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True python trainer.py \
  --model_size 1B \
  --mode production \
  --seq_len 2048 \
  --compile \
  --gradient_checkpointing \
  --triton_kernels \
  --chunked_ce \
  --chunked_ce_size 4096 \
  --8bit_adam \
  --dataset finefineweb-sequential
```
---

## ğŸ—„ï¸ Local Dataset Mounts (recommended)

If you store converted `.pt` shards on an external drive, avoid relying on GUI automount paths like `/media/<user>/<Drive Name>/...` (they can change, or be unavailable in non-GUI sessions).

HYDRA will use these environment variables when present:
- `HYDRA_DATA_ROOT`: A stable parent directory that contains `hydra_small_chat_pt/`, `hydra_nemotron_pt/`, and optionally `hf_finefineweb/`.
- `HYDRA_SMALL_CHAT_PT_DIR`: Explicit path to `hydra_small_chat_pt`.
- `HYDRA_NEMOTRON_PT_DIR`: Explicit path to `hydra_nemotron_pt`.

### Option A: Mount the drive to a stable path (best)

1) Find the drive UUID + filesystem type:
```bash
lsblk -f
```

2) Create a mount point (example):
```bash
sudo mkdir -p /mnt/hydra_data
```

3) Add an `/etc/fstab` entry using the UUID (edit with `sudo nano /etc/fstab`). Examples:

- **ext4**:
```text
UUID=<YOUR_UUID>  /mnt/hydra_data  ext4  defaults,nofail  0  2
```

- **exFAT** (common for portable SSDs):
```text
UUID=<YOUR_UUID>  /mnt/hydra_data  exfat  defaults,nofail,uid=1000,gid=1000,umask=022  0  0
```

4) Mount it:
```bash
sudo mount -a
```

5) Point HYDRA at the stable location:
```bash
export HYDRA_DATA_ROOT=/mnt/hydra_data
```

You can put the `export` into `~/.bashrc` or `~/.profile` to make it permanent.

### Option B: Symlink (quick, but less robust)

If you donâ€™t want to edit `fstab`, you can symlink the expected default paths to your current mount:
```bash
sudo mkdir -p /mnt/nvme0
sudo ln -s "/media/<user>/<Drive Name>/hydra_small_chat_pt" /mnt/nvme0/hydra_small_chat_pt
sudo ln -s "/media/<user>/<Drive Name>/hydra_nemotron_pt" /mnt/nvme0/hydra_nemotron_pt
```

---

## âš¡ Optional FP8 (Transformer Engine)

HYDRA includes optional integration with NVIDIA Transformer Engine (TE) to run **FP8** for *linear projections in CCGQA* when available.

- Default is **OFF** to avoid surprising numeric changes and because TE requires extra dependencies and Hopper+ GPUs.
- When enabled and supported, HYDRA will use TE's `fp8_autocast` + `TELinear` for the CCGQA module's `q/k/v/o` projections.

Requirements:
- Hopper+ GPU (sm_90+) and CUDA 12+
- `pip install transformer-engine[pytorch]`

Enable for CCGQA (opt-in):
- Set `te_fp8_projections=True` via the attention kwargs path.

Note:
- This only affects projection layers; the CCGQA attention computation runs in fp16/bf16.

## ğŸ§­ CCGQA Attention Implementation

All MoR blocks exclusively use **CCGQA (Compressed Convolutional Grouped Query Attention)** for consistency and optimal convergence.

The CCGQA implementation in each block:

1. **Compression Stage**: Compress input 4Ã— using a linear projection
2. **Convolution Layers**:
   - Causal sequence convolution (kernel=3) for local temporal dependencies
   - Pointwise (1Ã—1) channel convolution for cross-feature mixing
   - QK-mean coupling to stabilize gradient flow
3. **Attention Computation**:
   - Q, K, V projections from compressed input
   - L2 normalization of Q and K with learned temperature scaling
   - Grouped Query Attention (4:1 to 8:1 head sharing ratio)
   - Value shift: half of attention heads see the previous token
4. **Expansion**: Output expanded 4Ã— back to model dimension via linear projection

### Stepped Sequence Training (Advanced)

For 1B model with longer context, use stepped sequence scheduling:

| Phase | Seq Len | Batch | Accum | Memory | Tokens/Step |
|-------|---------|-------|-------|--------|-------------|
| **1** | 512 | 2 | 30 | 28.5GB | 30,720 |
| **2** | 1024 | 1 | 32 | ~25GB | 32,768 |
| **3** | 2048 | 1 | 32 | ~28GB | 65,536 |

---

## ğŸ”¬ Paper Compliance

### CCGQA (arXiv:2510.04476)

| Requirement | Implementation | Status |
|-------------|----------------|--------|
| 4Ã— compression factor | `compression_factor=4` | âœ… |
| Sequence convolutions | Causal 1D conv, kernel=3 | âœ… |
| Channel convolutions | Pointwise 1Ã—1 conv | âœ… |
| QK-mean coupling | Mean shared before/after conv | âœ… |
| QK L2 normalization | With learnable temperature | âœ… |
| GQA head sharing | `n_kv_heads < n_heads` | âœ… |
| Value shift | Half heads see previous token | âœ… |

### MoD (arXiv:2404.02258)

| Requirement | Implementation | Status |
|-------------|----------------|--------|
| Router architecture | Linear projection + sigmoid | âœ… |
| Soft routing (training) | Weighted sum by router probs | âœ… |
| Hard routing (inference) | Top-k selection, k=capacity | âœ… |
| 75% capacity target | `capacity_ratio=0.75` | âœ… |
| Auxiliary loss | BCE to maintain capacity | âœ… |
| Auto-scaling aux weight | `0.01 * (L/32) * âˆš(d/768)` | âœ… |

### MoR (arXiv:2507.10524)

| Requirement | Implementation | Status |
|-------------|----------------|--------|
| Gaussian soft routing | `N(Î¼, Ïƒ)` weighting over depths | âœ… |
| Layer-aware capacity | Early: 40%, Late: 80% | âœ… |
| Recursion embeddings | Additive depth embeddings | âœ… |
| Ponder loss | Encourages early stopping | âœ… |
| Depth histogram | Per-layer depth distribution | âœ… |

---

## ğŸ”€ Mixture of Experts (MoE)

HYDRA supports optional Mixture of Experts layers for increased model capacity with constant compute per token.

### MoE Architecture

MoE blocks are inserted as **separate FFN-only blocks** between existing transformer blocks:

```
[Transformer Block] â†’ [MoE Block] â†’ [Transformer Block] â†’ [MoE Block] â†’ ...
```

Each MoE block:
- Routes each token to `top_k` experts (default: top-1 routing)
- Uses auxiliary load-balancing loss (Switch-style)
- No token dropping (capacity factor = âˆ)
- torch.compile compatible (no graph breaks)

### CLI Flags

```bash
# Enable MoE
python trainer.py --model_size 500M --moe

# Configure MoE
python trainer.py \
    --moe \
    --moe_num_experts 8 \          # Number of expert FFNs (default: 4)
    --moe_num_layers 4 \           # How many MoE layers to insert
    --moe_top_k 2 \                # Experts per token (default: 1)
    --moe_aux_weight 0.01 \        # Load-balancing loss weight
    --moe_router_jitter 0.01 \     # Router noise during training
    --moe_warmup_steps 1000        # Dense warmup before routing
```

### Advanced MoE Options

```bash
# Domain-expert mapping (expert specialization)
--moe_domain_expert_map '{"code": 0, "math": 1}'

# Expert learning rate scaling
--moe_expert_lr_scale 0.5 \        # Lower LR for experts
--moe_router_lr_scale 2.0          # Higher LR for router

# Expert weight decay
--moe_expert_weight_decay_scale 0.1

# Teacher forcing for router training
--moe_teacher_weight 0.1 \
--moe_teacher_until_step 5000

# Divergence tracking
--moe_track_divergence \
--moe_divergence_interval 100
```

### MoE Scaling by Model Size

| Model | Experts | MoE Layers | Total Params | Active Params |
|-------|---------|------------|--------------|---------------|
| 250M | 4 | 2 | ~250M | ~198M |
| 500M | 4 | 4 | ~500M | ~400M |
| 1B | 8 | 6 | ~1.4B | ~973M |

> **Note**: Total params = base model + expert params. Active params = params used per forward pass.

---

## ğŸ”’ Static Routing Mode (CUDA Graph Compatibility)

HYDRA's MoD and MoR use dynamic routing by default, which is incompatible with CUDA graphs. For environments requiring static computation graphs, enable **static routing mode**:

```bash
python trainer.py --static_routing_mode
```

### What Changes in Static Mode

| Component | Dynamic Mode (default) | Static Mode |
|-----------|----------------------|-------------|
| **MoD** | Hard top-k selection | Soft weighted sum (all tokens) |
| **MoR** | Variable recursion depth | Fixed depth with soft weights |
| **CUDA Graphs** | âŒ Incompatible | âœ… Compatible |
| **Memory** | Lower (sparse) | Higher (dense) |
| **Speed** | Faster per-step | Faster launch overhead |

### When to Use Static Mode

- **Use dynamic mode** (default) for maximum training efficiency
- **Use static mode** when:
  - Deploying with CUDA graphs for inference
  - Profiling with consistent operation counts
  - Integration with systems requiring fixed computation graphs

---

## ğŸ“ Project Structure

```
HYDRA/
â”œâ”€â”€ hydra/                    # Main package
â”‚   â”œâ”€â”€ __init__.py          # Package exports
â”‚   â”œâ”€â”€ logging.py           # Logging utilities
â”‚   â”œâ”€â”€ utils.py             # Common utilities
â”‚   â”œâ”€â”€ attention/           # Attention backends
â”‚   â”‚   â”œâ”€â”€ backends/        
â”‚   â”‚   â”‚   â”œâ”€â”€ ccgqa/       # Compressed Convolutional GQA (primary)
â”‚   â”‚   â”‚   â””â”€â”€ lightning_attn3/  # [Archived] LA3 linear attention (legacy)
â”‚   â”‚   â””â”€â”€ factory.py       # Attention factory
â”‚   â”œâ”€â”€ data/                # Data loading utilities
â”‚   â”œâ”€â”€ kernels/             # Triton/CUDA kernels
â”‚   â”œâ”€â”€ layers/              # Core layer implementations
â”‚   â”œâ”€â”€ model/               # Model components
â”‚   â”‚   â”œâ”€â”€ framework/       # Model wiring (MoD/MoR + factories)
â”‚   â”‚   â””â”€â”€ ccgqa/           # Back-compat shims
â”‚   â”œâ”€â”€ optim/               # Optimizers and schedulers
â”‚   â”œâ”€â”€ routing/             # Routing modules (MoD, MoR)
â”‚   â””â”€â”€ training/            # Training infrastructure
â”‚       â”œâ”€â”€ trainer.py       # Main trainer class
â”‚       â”œâ”€â”€ config.py        # Configuration dataclasses
â”‚       â”œâ”€â”€ checkpointing.py # Checkpoint management
â”‚       â””â”€â”€ metrics.py       # Training metrics
â”œâ”€â”€ trainer.py               # Training entrypoint (CLI)
â”œâ”€â”€ scripts/                 # Utility scripts
â”‚   â”œâ”€â”€ compare_mod_mor_effectiveness.py  # MoD/MoR comparison
â”‚   â””â”€â”€ ...
â”œâ”€â”€ tests/                   # Test suite (305 tests)
â”‚   â””â”€â”€ test_paper_compliance.py  # Paper compliance tests
â”œâ”€â”€ diagnostics/             # Diagnostic and benchmarking tools
â”‚   â”œâ”€â”€ mod_mor_routing_healthcheck.py  # Routing health checks
â”‚   â”œâ”€â”€ scaling_analysis.py  # Multi-scale analysis
â”‚   â””â”€â”€ ...
â”œâ”€â”€ configs/                 # Model configurations
â”‚   â””â”€â”€ variants.yaml        # Model variant definitions
â”œâ”€â”€ reports/                 # Generated analysis reports
â”œâ”€â”€ checkpoints/             # Training checkpoints (hydra_{model_size}_*.pt)
â”œâ”€â”€ README.md                # This file
â”œâ”€â”€ pytest.ini               # Test configuration
â””â”€â”€ requirements.txt         # Dependencies
```

---

## ğŸ¯ MoD/MoR Curriculum Training

HYDRA uses a **curriculum approach** for MoD and MoR to ensure stable training:

### MoD (Mixture of Depths) Curriculum

| Phase | Step Range | Behavior |
|-------|-----------|----------|
| **Warmup** | 0 â†’ 10% | MoD **disabled** (dense MLP, all tokens processed) |
| **Loss Gate** | 10% â†’ force% | MoD enables when CE loss EMA < 5.0 |
| **Force Enable** | 15-20% | MoD **forced on** regardless of loss |
| **Active** | 20% â†’ 100% | MoD active, ~50% compute savings |

### MoR (Mixture of Recursions) Curriculum

| Phase | Step Range | Behavior |
|-------|-----------|----------|
| **Fixed Depth** | 0 â†’ 20-30% | All tokens use maximum recursion depth |
| **Ramp Up** | 20% â†’ 30% | Gradually enable adaptive depth routing |
| **Full Adaptive** | 30% â†’ 100% | MoR decides recursion depth per-token |

### Running Curriculum Experiments

```bash
# Standard curriculum (MoD@10%, MoR@30%)
# CCGQA attention is the default
python trainer.py --model_size 50M --max_steps 5000

# Override curriculum timing (for short experiments)
python trainer.py --model_size 50M --max_steps 1000 \
    --no_short_run_override \
    --mod_enable_pct 0.10 --mod_force_enable_pct 0.15 \
    --mor_enable_pct 0.20

# Disable MoD/MoR (vanilla baseline with CCGQA)
python trainer.py --model_size 50M --max_steps 5000 \
    --mod_off --mor_off
```

### MoD/MoR Effectiveness Comparison

Run the comprehensive comparison script to evaluate routing effectiveness:

```bash
# Full comparison: vanilla vs MoD-only vs MoR-only vs full routing
python scripts/compare_mod_mor_effectiveness.py --model_size 50M --max_steps 5000

# Quick test (1000 steps)
python scripts/compare_mod_mor_effectiveness.py --model_size 50M --max_steps 1000 --quick
```

This generates:
- **Loss curves** comparing all configurations
- **MoD compute savings** per layer
- **MoR depth histograms** (token distribution across recursion levels)
- **Summary report** with key findings

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourname/hydra.git
cd hydra

# Install dependencies
pip install -r requirements.txt

# Install in development mode
pip install -e .
```

### Basic Usage

```python
from hydra import create_ccgqa_mod_mor_model

# Create a 100M model
model = create_ccgqa_mod_mor_model(
    vocab_size=32000,
    dim=768,
    n_mor_blocks=8,
    recursions=4,
    n_heads=12,
    n_kv_heads=3,
    compression_factor=4,
    capacity_ratio=0.75,
)

# Forward pass
input_ids = torch.randint(0, 32000, (1, 512))
outputs = model(input_ids)

logits = outputs["logits"]           # [batch, seq, vocab]
aux_loss = outputs["aux_loss"]       # MoD capacity loss
ponder_loss = outputs["ponder_loss"] # MoR depth loss

# Total loss for training
total_loss = ce_loss + 0.01 * aux_loss + 0.01 * ponder_loss
```

### Run Tests

```bash
# Run full compliance test suite (64 tests)
pytest tests/test_paper_compliance.py -v

# Run fast tests only
pytest tests/test_paper_compliance.py -v -m "not slow"
```

### Run Diagnostics

```bash
# Run scaling analysis across all variants
python diagnostics/scaling_analysis.py \
    --variants 100M 250M 500M 750M 900M 1B 1.5B \
    --steps 30 \
    --plot \
    --predict-4b \
    --output reports/scaling_analysis_results.json
```

### Attention Architecture (MoR blocks)

HYDRA uses **CCGQA (Compressed Convolutional Grouped Query Attention)** for all MoR blocks to provide stable, efficient attention computation.

**Performance Characteristics**: 
- Memory efficient: KV cache reduction through 4:1 to 8:1 GQA head sharing
- Stable convergence: 16Ã— fewer attention operations through 4Ã— compression
- Proven results: Validated across 100M to 1B model scales

```bash
# Run scaling analysis across all model variants
python diagnostics/scaling_analysis.py \
    --variants 100M 250M 500M 750M 900M 1B 1.5B \
    --steps 30 \
    --plot \
    --predict-4b \
    --output reports/scaling_analysis_results.json
```

---

## ğŸ“ˆ Scaling Analysis

The architecture has been validated across 7 model scales with curve fitting to predict 4B behavior:

### Curve Fitting Results

| Metric | Best Fit | RÂ² | 4B Prediction |
|--------|----------|-----|---------------|
| `aux_loss_weight` | Polynomial (deg 2) | 0.990 | ~0.102 |
| `mod_prob` | Polynomial (deg 2) | 0.901 | Stable ~0.75 |
| `mor_depth` | Constant | 1.000 | 1.0 |
| Compute time | Polynomial (deg 2) | 0.951 | ~47s/step |

### Auto-Scaling Formula

For large models, the auxiliary loss weight automatically scales:

```python
aux_loss_weight = 0.01 * (effective_layers / 32) * sqrt(dim / 768)
```

This ensures MoD capacity remains at 75% even as model size increases.

---

## ğŸ§ª Testing Philosophy

HYDRA follows a rigorous testing philosophy:

1. **Paper Compliance**: Every architectural claim is validated against the source papers
2. **Scale Invariance**: Tests run at multiple scales (100M â†’ 1.5B)
3. **Repeatability**: All tests use fixed seeds and are deterministic
4. **Regression Prevention**: Scaling analysis detects drift in hyperparameters

---

## âš™ï¸ Performance Optimizations

### Kernel-Level Optimizations

**Liger Kernels (BF16 fused operations, auto-enabled)**
- **LigerRMSNorm**: ~30% memory savings, 1.5-2Ã— faster
- **LigerSwiGLU**: ~1.3Ã— faster, avoids intermediate materialization
- **LigerCrossEntropy**: ~60% memory savings, 2Ã— faster
- **LigerFusedLinearCrossEntropy**: ~80% output layer savings, never materializes full logits

**Triton Custom Kernels (opt-in via `--triton_kernels`)**

| Kernel | Speedup | Forward | Backward | Notes |
|--------|---------|---------|----------|-------|
| **fused_qk_norm** | 1.5-2Ã— | âœ… | âœ… | L2 norm for Q/K with fused backward |
| **fused_swiglu** | 1.3Ã— | âœ… | âœ… | Fused gate*up with single-kernel backward |
| **fused_rms_norm** | 1.5Ã— | âœ… | âœ… | Fused normalization with backward |
| **fused_rope** | 2-3Ã— | âœ… | âŒ | RoPE (forward only, backward via PyTorch) |

**Fused Backward Kernels (New)**

The fused backward kernels reduce kernel launch overhead dramatically:
- **SwiGLU backward**: ~12 kernel launches â†’ 1 fused kernel
- **RMSNorm backward**: ~6 kernel launches â†’ 1 fused kernel
- **QK-Norm backward**: ~8 kernel launches â†’ 1 fused kernel

All fused backward kernels are **enabled by default** when `--triton_kernels` is set.

**Flash Attention**
- Flash Attention 2/3 auto-detected and enabled
- Memory-efficient attention (no QK^T materialization)
- FP8 support on Flash Attention 3

### Training Infrastructure

**torch.compile**: Graph optimization with `max-autotune-no-cudagraphs` mode
**Mixed Precision**: BF16 forward/backward with FP32 master weights
**Memory Optimization**:
- Gradient checkpointing (every N layers, default N=2)
- 8-bit Adam (~75% optimizer memory savings, essential for 750M+)
- Chunked cross-entropy (4096 tokens per chunk)

**Data Loading**:
- Multi-worker parallel loading (4-8Ã— faster)
- Background prefetching (2Ã— prefetch factor)
- Rust-based fast tokenizers (3-10Ã— faster)
- HF Transfer protocol (5-10Ã— faster downloads)

**Learning Rate**:
- WSD (Warmup-Stable-Decay) scheduler with adaptive LR
- Auto-trigger cooldown on loss spikes
- Stochastic Weight Averaging (last 25% of training)
- Batch filtering (skip corrupted/noisy batches)

### Environment Variables

**Triton Kernel Controls** (all enabled by default when `--triton_kernels` is set):

| Variable | Default | Description |
|----------|---------|-------------|
| `HYDRA_DISABLE_TRITON` | `0` | Disable all Triton kernels globally |
| `HYDRA_ENABLE_FUSED_ROPE` | `1` | Enable fused RoPE kernel |
| `HYDRA_DISABLE_FUSED_ROPE` | `0` | Force-disable fused RoPE |
| `HYDRA_ENABLE_FUSED_RMS_NORM` | `1` | Enable fused RMSNorm forward |
| `HYDRA_DISABLE_FUSED_RMS_NORM` | `0` | Force-disable fused RMSNorm |
| `HYDRA_ENABLE_FUSED_RMS_NORM_BWD` | `1` | Enable fused RMSNorm backward |
| `HYDRA_DISABLE_FUSED_RMS_NORM_BWD` | `0` | Force-disable fused RMSNorm backward |
| `HYDRA_ENABLE_FUSED_SWIGLU_BWD` | `1` | Enable fused SwiGLU backward |
| `HYDRA_DISABLE_FUSED_SWIGLU_BWD` | `0` | Force-disable fused SwiGLU backward |
| `HYDRA_ENABLE_FUSED_QK_NORM_BWD` | `1` | Enable fused QK-Norm backward |
| `HYDRA_DISABLE_FUSED_QK_NORM_BWD` | `0` | Force-disable fused QK-Norm backward |

**Liger Kernel Controls**:

| Variable | Default | Description |
|----------|---------|-------------|
| `HYDRA_ENABLE_LIGER_CE` | `1` | Enable Liger fused cross-entropy (if available) |
| `HYDRA_DISABLE_LIGER_CE` | `0` | Force-disable Liger cross-entropy |

**Other Settings**:

| Variable | Default | Description |
|----------|---------|-------------|
| `HYDRA_CCQA_USE_FUSED_KERNEL` | `0` | Enable fused CCGQA kernel for attention (experimental) |
| `HF_HUB_ENABLE_HF_TRANSFER` | `1` | Enable fast HuggingFace transfers (auto-enabled) |

```bash
# Disable specific fused backward kernels (for debugging)
export HYDRA_DISABLE_FUSED_SWIGLU_BWD=1
export HYDRA_DISABLE_FUSED_RMS_NORM_BWD=1
python trainer.py --triton_kernels ...

# Disable all Triton kernels
export HYDRA_DISABLE_TRITON=1
python trainer.py ...
```

---

## ğŸ“š References

```bibtex
@article{ccgqa2024,
  title={Compressed Convolutional Attention: Efficient Attention in a Compressed Latent Space},
  author={...},
  journal={arXiv preprint arXiv:2510.04476},
  year={2024}
}

@article{mod2024,
  title={Mixture-of-Depths: Dynamically allocating compute in transformer-based language models},
  author={Raposo, David and others},
  journal={arXiv preprint arXiv:2404.02258},
  year={2024}
}

@article{mor2025,
  title={Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation},
  author={...},
  journal={arXiv preprint arXiv:2507.10524},
  year={2025}
}
```

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) for details.

---

## ğŸ¤ Contributing

Contributions welcome!

---

<p align="center">
  <strong>HYDRA</strong> - Multi-headed efficiency for modern transformers ğŸ‰
</p>
